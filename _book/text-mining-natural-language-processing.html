<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Text mining &amp; natural language processing | DataManagement.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Text mining &amp; natural language processing | DataManagement.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Text mining &amp; natural language processing | DataManagement.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-visualization-1.html"/>
<link rel="next" href="cluster-computing.html"/>
<script src="libs/header-attrs-2.12/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="data-management-databases-and-organizations.html#data-management-databases-and-organizations">Data Management: Databases and Organizations<span></span></a></li>
<li><a href="preface.html#preface">Preface<span></span></a>
<ul>
<li><a href="preface.html#supplements">Supplements<span></span></a></li>
<li><a href="preface.html#acknowledgments">Acknowledgments<span></span></a></li>
</ul></li>
<li><a href="section-1-the-managerial-perspective.html#section-1-the-managerial-perspective">Section 1 The Managerial Perspective<span></span></a></li>
<li class="chapter" data-level="1" data-path="managing-data.html"><a href="managing-data.html"><i class="fa fa-check"></i><b>1</b> Managing Data<span></span></a>
<ul>
<li><a href="managing-data.html#introduction">Introduction<span></span></a></li>
<li><a href="managing-data.html#individual-data-management">Individual data management<span></span></a></li>
<li><a href="managing-data.html#organizational-data-management">Organizational data management<span></span></a></li>
<li><a href="managing-data.html#problems-with-data-management-systems">Problems with data management systems<span></span></a></li>
<li><a href="managing-data.html#a-brief-history-of-data-management-systems">A brief history of data management systems<span></span></a></li>
<li><a href="managing-data.html#data-information-and-knowledge">Data, information, and knowledge<span></span></a></li>
<li><a href="managing-data.html#the-challenge">The challenge<span></span></a></li>
<li><a href="managing-data.html#exercises">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="information.html"><a href="information.html"><i class="fa fa-check"></i><b>2</b> Information<span></span></a>
<ul>
<li><a href="information.html#introduction-1">Introduction<span></span></a></li>
<li><a href="information.html#a-historical-perspective">A historical perspective<span></span></a></li>
<li><a href="information.html#a-brief-history-of-information-systems">A brief history of information systems<span></span></a></li>
<li><a href="information.html#information-characteristics">Information characteristics<span></span></a></li>
<li><a href="information.html#information-and-organizational-change">Information and organizational change<span></span></a></li>
<li><a href="information.html#information-and-managerial-work">Information and managerial work<span></span></a></li>
<li><a href="information.html#managers-information-requirements">Managers’ information requirements<span></span></a></li>
<li><a href="information.html#information-delivery-systems">Information delivery systems<span></span></a></li>
<li><a href="information.html#information-integration">Information integration<span></span></a></li>
<li><a href="information.html#knowledge-1">Knowledge<span></span></a></li>
<li><a href="information.html#exercises-1">Exercises<span></span></a></li>
</ul></li>
<li><a href="section-2-data-modeling-and-sql.html#section-2-data-modeling-and-sql">Section 2 Data Modeling and SQL<span></span></a></li>
<li class="chapter" data-level="3" data-path="the-single-entity.html"><a href="the-single-entity.html"><i class="fa fa-check"></i><b>3</b> The Single Entity<span></span></a>
<ul>
<li><a href="the-single-entity.html#the-relational-model">The relational model<span></span></a></li>
<li><a href="the-single-entity.html#getting-started">Getting started<span></span></a></li>
<li><a href="the-single-entity.html#modeling-a-single-entity-database">Modeling a single-entity database<span></span></a></li>
<li><a href="the-single-entity.html#creating-a-single-table-database">Creating a single-table database<span></span></a></li>
<li><a href="the-single-entity.html#querying-a-single-table-database">Querying a single-table database<span></span></a></li>
<li><a href="the-single-entity.html#exercises-2">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html"><i class="fa fa-check"></i><b>4</b> The One-to-Many Relationship<span></span></a>
<ul>
<li><a href="the-one-to-many-relationship.html#relationships">Relationships<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#creating-a-database-with-a-1m-relationship">Creating a database with a 1:m relationship<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#querying-a-two-table-database">Querying a two-table database<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#regular-expressionpattern-matching-1">Regular expression—pattern matching<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#subqueries-1">Subqueries<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#exercises-3">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html"><i class="fa fa-check"></i><b>5</b> The Many-to-Many Relationship<span></span></a>
<ul>
<li><a href="the-many-to-many-relationship.html#the-many-to-many-relationship-1">The many-to-many relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#creating-a-relational-database-with-an-mm-relationship">Creating a relational database with an m:m relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#querying-an-mm-relationship">Querying an m:m relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#key-terms-and-concepts-2">Key terms and concepts<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#exercises-4">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html"><i class="fa fa-check"></i><b>6</b> One-to-One and Recursive Relationships<span></span></a>
<ul>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-one-to-one-relationship">Modeling a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-one-to-one-relationship">Mapping a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-many-relationship">Mapping a recursive one-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-one-to-one-relationship">Querying a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-1m-relationship">Querying a recursive 1:m relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-one-to-one-relationship">Modeling a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-one-relationship">Mapping a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-one-to-one-relationship">Querying a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-many-to-many-relationship">Modeling a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-many-to-many-relationship">Mapping a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-many-to-many-relationship">Querying a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#exercises-5">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-modeling.html"><a href="data-modeling.html"><i class="fa fa-check"></i><b>7</b> Data Modeling<span></span></a>
<ul>
<li><a href="data-modeling.html#modeling">Modeling<span></span></a></li>
<li><a href="data-modeling.html#data-modeling-1">Data modeling<span></span></a></li>
<li><a href="data-modeling.html#data-model-quality">Data model quality<span></span></a></li>
<li><a href="data-modeling.html#quality-improvement">Quality improvement<span></span></a></li>
<li><a href="data-modeling.html#data-modeling-hints">Data modeling hints<span></span></a></li>
<li><a href="data-modeling.html#the-seven-habits-of-highly-effective-data-modelers">The seven habits of highly effective data modelers<span></span></a></li>
<li><a href="data-modeling.html#exercises-6">Exercises<span></span></a></li>
</ul></li>
<li><a href="reference-1-basic-structures.html#reference-1-basic-structures">Reference 1: Basic Structures<span></span></a>
<ul>
<li><a href="reference-1-basic-structures.html#one-entity">One entity<span></span></a></li>
<li><a href="reference-1-basic-structures.html#two-entities">Two entities<span></span></a></li>
<li><a href="reference-1-basic-structures.html#another-entitys-identifier-as-part-of-the-identifier">Another entity’s identifier as part of the identifier<span></span></a></li>
<li><a href="reference-1-basic-structures.html#exercises-7">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html"><i class="fa fa-check"></i><b>8</b> Normalization and Other Data Modeling Methods<span></span></a>
<ul>
<li><a href="normalization-and-other-data-modeling-methods.html#multiple-paths">Multiple paths<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#normalization">Normalization<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#other-data-modeling-methods">Other data modeling methods<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#key-terms-and-concepts-5">Key terms and concepts<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#exercises-8">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html"><i class="fa fa-check"></i><b>9</b> The Relational Model and Relational Algebra<span></span></a>
<ul>
<li><a href="the-relational-model-and-relational-algebra.html#background">Background<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#data-structures">Data structures<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#integrity-rules">Integrity rules<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#manipulation-languages">Manipulation languages<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#a-fully-relational-database">A fully relational database<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#exercises-9">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="sql.html"><a href="sql.html"><i class="fa fa-check"></i><b>10</b> SQL<span></span></a>
<ul>
<li><a href="sql.html#structured-query-language">Structured query language<span></span></a></li>
<li><a href="sql.html#creating-a-table">Creating a table<span></span></a></li>
<li><a href="sql.html#data-types">Data types<span></span></a></li>
<li><a href="sql.html#collation-sequence">Collation sequence<span></span></a></li>
<li><a href="sql.html#scalar-functions">Scalar functions<span></span></a></li>
<li><a href="sql.html#formatting">Formatting<span></span></a></li>
<li><a href="sql.html#table-commands">Table commands<span></span></a></li>
<li><a href="sql.html#data-manipulation">Data manipulation<span></span></a></li>
<li><a href="sql.html#insert">INSERT<span></span></a></li>
<li><a href="sql.html#update-1">UPDATE<span></span></a></li>
<li><a href="sql.html#delete-1">DELETE<span></span></a></li>
<li><a href="sql.html#sql-routines">SQL routines<span></span></a></li>
<li><a href="sql.html#universal-unique-identifier-uuid">Universal Unique Identifier (UUID)<span></span></a></li>
<li><a href="sql.html#nullsmuch-ado-about-missing-information">Nulls—much ado about missing information<span></span></a></li>
<li><a href="sql.html#security">Security<span></span></a></li>
<li><a href="sql.html#the-system-catalog">The system catalog<span></span></a></li>
<li><a href="sql.html#natural-language-processing">Natural language processing<span></span></a></li>
<li><a href="sql.html#connectivity-and-odbc">Connectivity and ODBC<span></span></a></li>
<li><a href="sql.html#embedded-sql">Embedded SQL<span></span></a></li>
<li><a href="sql.html#user-defined-types">User-defined types<span></span></a></li>
<li><a href="sql.html#the-future-of-sql">The future of SQL<span></span></a></li>
</ul></li>
<li><a href="reference-2-sql-playbook.html#reference-2-sql-playbook">Reference 2: SQL Playbook<span></span></a>
<ul>
<li><a href="reference-2-sql-playbook.html#the-power-of-sql">The power of SQL<span></span></a></li>
</ul></li>
<li><a href="section-3-advanced-data-management.html#section-3-advanced-data-management">Section 3 Advanced Data Management<span></span></a></li>
<li class="chapter" data-level="11" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html"><i class="fa fa-check"></i><b>11</b> Spatial and Temporal Data Management<span></span></a>
<ul>
<li><a href="spatial-and-temporal-data-management.html#spatial-data">Spatial data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#managing-spatial-data">Managing spatial data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#data-model-mapping">Data model mapping<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#r-tree">R-tree<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#managing-temporal-data">Managing temporal data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#key-terms-and-concepts-8">Key terms and concepts<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#exercises-11">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="graph-databases.html"><a href="graph-databases.html"><i class="fa fa-check"></i><b>12</b> Graph Databases<span></span></a>
<ul>
<li><a href="graph-databases.html#a-graph-database">A graph database<span></span></a></li>
<li><a href="graph-databases.html#neo4j-a-graph-database-implementation">Neo4j – a graph database implementation<span></span></a></li>
<li><a href="graph-databases.html#a-relationship-between-nodes">A relationship between nodes<span></span></a></li>
<li><a href="graph-databases.html#key-terms-and-concepts-9">Key terms and concepts<span></span></a></li>
<li><a href="graph-databases.html#exercises-12">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="xml-managing-data-exchange.html"><a href="xml-managing-data-exchange.html"><i class="fa fa-check"></i><b>13</b> XML: Managing Data Exchange<span></span></a>
<ul>
<li><a href="xml-managing-data-exchange.html#four-problems">Four problems<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#sgml">SGML<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#xml">XML<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#xml-schema">XML schema<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html"><i class="fa fa-check"></i><b>14</b> Organizational Intelligence<span></span></a>
<ul>
<li><a href="organizational-intelligence.html#information-poverty">Information poverty<span></span></a></li>
<li><a href="organizational-intelligence.html#an-organizational-intelligence-system">An organizational intelligence system<span></span></a></li>
<li><a href="organizational-intelligence.html#the-data-warehouse">The data warehouse<span></span></a></li>
<li><a href="organizational-intelligence.html#exploiting-data-stores">Exploiting data stores<span></span></a></li>
<li><a href="organizational-intelligence.html#data-mining">Data mining<span></span></a></li>
<li><a href="organizational-intelligence.html#exercises-14">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>15</b> Introduction to R<span></span></a>
<ul>
<li><a href="introduction-to-r.html#the-r-project">The R project<span></span></a></li>
<li><a href="introduction-to-r.html#datasets">Datasets<span></span></a></li>
<li><a href="introduction-to-r.html#packages">Packages<span></span></a></li>
<li><a href="introduction-to-r.html#file-handing">File handing<span></span></a></li>
<li><a href="introduction-to-r.html#data-manipulation-with-dplyr">Data manipulation with dplyr<span></span></a></li>
<li><a href="introduction-to-r.html#database-access">Database access<span></span></a></li>
<li><a href="introduction-to-r.html#excel-files">Excel files<span></span></a></li>
<li><a href="introduction-to-r.html#r-resources">R resources<span></span></a></li>
<li><a href="introduction-to-r.html#r-and-data-analytics">R and data analytics<span></span></a></li>
<li><a href="introduction-to-r.html#exercises-15">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="data-visualization-1.html"><a href="data-visualization-1.html"><i class="fa fa-check"></i><b>16</b> Data visualization<span></span></a>
<ul>
<li><a href="data-visualization-1.html#visual-processing">Visual processing<span></span></a></li>
<li><a href="data-visualization-1.html#the-grammar-of-graphics">The grammar of graphics<span></span></a></li>
<li><a href="data-visualization-1.html#ggplot2">ggplot2<span></span></a></li>
<li><a href="data-visualization-1.html#some-recipes">Some recipes<span></span></a></li>
<li><a href="data-visualization-1.html#geographic-data">Geographic data<span></span></a></li>
<li><a href="data-visualization-1.html#r-resources-1">R resources<span></span></a></li>
<li><a href="data-visualization-1.html#exercises-16">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html"><i class="fa fa-check"></i><b>17</b> Text mining &amp; natural language processing<span></span></a>
<ul>
<li><a href="text-mining-natural-language-processing.html#the-nature-of-language">The nature of language<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#levels-of-processing">Levels of processing<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#tokenization">Tokenization<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#sentiment-analysis">Sentiment analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#corpus">Corpus<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#readability">Readability<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#preprocessing">Preprocessing<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#word-frequency-analysis">Word frequency analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#co-occurrence-and-association">Co-occurrence and association<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#cluster-analysis">Cluster analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#topic-modeling">Topic modeling<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#named-entity-recognition-ner">Named-entity recognition (NER)<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#future-developments">Future developments<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#key-terms-and-concepts-13">Key terms and concepts<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#exercises-17">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="cluster-computing.html"><a href="cluster-computing.html"><i class="fa fa-check"></i><b>18</b> Cluster computing<span></span></a>
<ul>
<li><a href="cluster-computing.html#a-paradigm-shift">A paradigm shift<span></span></a></li>
<li><a href="cluster-computing.html#the-drivers">The drivers<span></span></a></li>
<li><a href="cluster-computing.html#the-bottleneck-and-its-solution">The bottleneck and its solution<span></span></a></li>
<li><a href="cluster-computing.html#lambda-architecture">Lambda Architecture<span></span></a></li>
<li><a href="cluster-computing.html#hadoop">Hadoop<span></span></a></li>
<li><a href="cluster-computing.html#spark">Spark<span></span></a></li>
<li><a href="cluster-computing.html#exercises-18">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="dashboards.html"><a href="dashboards.html"><i class="fa fa-check"></i><b>19</b> Dashboards<span></span></a>
<ul>
<li><a href="dashboards.html#the-value-of-dashboards">The value of dashboards<span></span></a></li>
<li><a href="dashboards.html#designing-a-dashboard">Designing a dashboard<span></span></a></li>
<li><a href="dashboards.html#dashboards-with-r">Dashboards with R<span></span></a></li>
<li><a href="dashboards.html#conclusion-5">Conclusion<span></span></a></li>
<li><a href="dashboards.html#exercises-19">Exercises<span></span></a></li>
</ul></li>
<li><a href="section-4-managing-organizational-memory.html#section-4-managing-organizational-memory">Section 4 Managing Organizational Memory<span></span></a></li>
<li class="chapter" data-level="20" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html"><i class="fa fa-check"></i><b>20</b> Data Structure and Storage<span></span></a>
<ul>
<li><a href="data-structure-and-storage.html#the-data-deluge">The data deluge<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-structures-1">Data structures<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-coding-standards">Data coding standards<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-storage-devices">Data storage devices<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-compression">Data compression<span></span></a></li>
<li><a href="data-structure-and-storage.html#key-terms-and-concepts-15">Key terms and concepts<span></span></a></li>
<li><a href="data-structure-and-storage.html#exercises-20">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html"><i class="fa fa-check"></i><b>21</b> Data Processing Architectures<span></span></a>
<ul>
<li><a href="data-processing-architectures.html#architectural-choices">Architectural choices<span></span></a></li>
<li><a href="data-processing-architectures.html#remote-job-entry">Remote job entry<span></span></a></li>
<li><a href="data-processing-architectures.html#personal-database">Personal database<span></span></a></li>
<li><a href="data-processing-architectures.html#clientserver">Client/server<span></span></a></li>
<li><a href="data-processing-architectures.html#cloud-computing">Cloud computing<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-database">Distributed database<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-data-access">Distributed data access<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-database-design">Distributed database design<span></span></a></li>
<li><a href="data-processing-architectures.html#key-terms-and-concepts-16">Key terms and concepts<span></span></a></li>
<li><a href="data-processing-architectures.html#exercises-21">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="sql-and-java.html"><a href="sql-and-java.html"><i class="fa fa-check"></i><b>22</b> SQL and Java<span></span></a>
<ul>
<li><a href="sql-and-java.html#java">JAVA<span></span></a></li>
<li><a href="sql-and-java.html#using-sql-within-java">Using SQL within Java<span></span></a></li>
<li><a href="sql-and-java.html#javaserver-pages-jsp">JavaServer Pages (JSP)<span></span></a></li>
<li><a href="sql-and-java.html#exercises-22">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-integrity.html"><a href="data-integrity.html"><i class="fa fa-check"></i><b>23</b> Data Integrity<span></span></a>
<ul>
<li><a href="data-integrity.html#introduction-2">Introduction<span></span></a></li>
<li><a href="data-integrity.html#transaction-management">Transaction management<span></span></a></li>
<li><a href="data-integrity.html#protecting-existence">Protecting existence<span></span></a></li>
<li><a href="data-integrity.html#maintaining-data-quality">Maintaining data quality<span></span></a></li>
<li><a href="data-integrity.html#ensuring-confidentiality">Ensuring confidentiality<span></span></a></li>
<li><a href="data-integrity.html#key-terms-and-concepts-18">Key terms and concepts<span></span></a></li>
<li><a href="data-integrity.html#exercises-23">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="data-administration.html"><a href="data-administration.html"><i class="fa fa-check"></i><b>24</b> Data Administration<span></span></a>
<ul>
<li><a href="data-administration.html#introduction-3">Introduction<span></span></a></li>
<li><a href="data-administration.html#the-chief-data-officer">The Chief Data Officer<span></span></a></li>
<li><a href="data-administration.html#management-of-the-database-environment">Management of the database environment<span></span></a></li>
<li><a href="data-administration.html#data-administration-1">Data administration<span></span></a></li>
<li><a href="data-administration.html#database-management-systems-dbmss">Database management systems (DBMSs)<span></span></a></li>
<li><a href="data-administration.html#groupware-1">Groupware<span></span></a></li>
<li><a href="data-administration.html#data-integration">Data integration<span></span></a></li>
<li><a href="data-administration.html#conclusion-9">Conclusion<span></span></a></li>
<li><a href="data-administration.html#exercises-24">Exercises<span></span></a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="text-mining-natural-language-processing" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Chapter 17</span> Text mining &amp; natural language processing<a href="text-mining-natural-language-processing.html#text-mining-natural-language-processing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p>From now on I will consider a language to be a set (finite or infinite) of sentences, each finite in length and constructed out of a finite set of elements. All natural languages in their spoken or written form are languages in this sense.</p>
<p>Noam Chomsky, <em>Syntactic Structures</em></p>
</blockquote>
<div id="learning-objectives-16" class="section level3 unnumbered hasAnchor">
<h3>Learning objectives<a href="text-mining-natural-language-processing.html#learning-objectives-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Students completing this chapter will:</p>
<ul>
<li><p>Have a realistic understanding of the capabilities of current text mining and NLP software;</p></li>
<li><p>Be able to use R and associated packages for text mining and NLP.</p></li>
</ul>
</div>
<div id="the-nature-of-language" class="section level2 unnumbered hasAnchor">
<h2>The nature of language<a href="text-mining-natural-language-processing.html#the-nature-of-language" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Language enables humans to cooperate through information exchange. We typically associate language with sound and writing, but gesturing, which is older than speech, is also a means of collaboration. The various dialects of sign languages are effective tools for visual communication. Some species, such as ants and bees, exchange information using chemical substances known as pheromones. Of all the species, humans have developed the most complex system for cooperation, starting with gestures and progressing to digital technology, with language being the core of our ability to work collectively.</p>
<p>Natural language processing (NLP) focuses on developing and implementing software that enables computers to handle large scale processing of language in a variety of forms, such as written and spoken. While it is a relatively easy task for computers to process numeric information, language is far more difficult because of the flexibility with which it is used, even when grammar and syntax are precisely obeyed. There is an inherent ambiguity of written and spoken speech. For example, the word “set” can be a noun, verb, or adjective, and the <em>Oxford English Dictionary</em> defines over 40 different meanings. Irregularities in language, both in its structure and use, and ambiguities in meaning make NLP a challenging task. Be forewarned. Don’t expect NLP to provide the same level of exactness and starkness as numeric processing. NLP output can be messy, imprecise, and confusing – just like the language that goes into an NLP program. One of the well-known maxims of information processing is “garbage-in, garbage-out.” While language is not garbage, we can certainly observe that “ambiguity-in, ambiguity-out” is a truism. You can’t start with something that is marginally ambiguous and expect a computer to turn it into a precise statement. Legal and religious scholars can spend years learning how to interpret a text and still reach different conclusions as to its meaning.</p>
<p>NLP, despite its limitations, enables humans to process large volumes of language data (e.g., text) quickly and to identify patterns and features that might be useful. A well-educated human with domain knowledge specific to the same data might make more sense of these data, but it might take months or years. For example, a firm might receive over a 1,000 tweets, 500 Facebook mentions, and 20 blog references in a day. It needs NLP to identify within minutes or hours which of these many messages might need human action.</p>
<p>Text mining and NLP overlap in their capabilities and goals. The ultimate objective is to extract useful and valuable information from text using analytical methods and NLP. Simply counting words in a document is a an example of text mining because it requires minimal NLP technology, other than separating text into words. Whereas, recognizing entities in a document requires prior extensive machine learning and more intensive NLP knowledge. Whether you call it text mining or NLP, you are processing natural language. We will use the terms somewhat interchangeably in this chapter.</p>
<p>The human brain has a special capability for learning and processing languages and reconciling ambiguities,<a href="data-administration.html#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> and it is a skill we have yet to transfer to computers. NLP can be a good servant, but enter its realm with realistic expectations of what is achievable with the current state-of-the-art.</p>
</div>
<div id="levels-of-processing" class="section level2 unnumbered hasAnchor">
<h2>Levels of processing<a href="text-mining-natural-language-processing.html#levels-of-processing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are three levels to consider when processing language.</p>
<div id="semantics" class="section level3 unnumbered hasAnchor">
<h3>Semantics<a href="text-mining-natural-language-processing.html#semantics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Semantics focuses on the meaning of words and the interactions between words to form larger units of meaning (such as sentences). Words in isolation often provide little information. We normally need to read or hear a sentence to understand the sender’s intent. One word can change the meaning of a sentence (e.g., “Help needed versus Help not needed”). It is typically an entire sentence that conveys meaning. Of course, elaborate ideas or commands can require many sentences.</p>
</div>
<div id="discourse" class="section level3 unnumbered hasAnchor">
<h3>Discourse<a href="text-mining-natural-language-processing.html#discourse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Building on semantic analysis, discourse analysis aims to determine the relationships between sentences in a communication, such as a conversation, consisting of multiple sentences in a particular order. Most human communications are a series of connected sentences that collectively disclose the sender’s goals. Typically, interspersed in a conversation are one or more sentences from one or more receivers as they try to understand the sender’s purpose and maybe interject their thoughts and goals into the discussion. The points and counterpoints of a blog are an example of such a discourse. As you might imagine, making sense of discourse is frequently more difficult, for both humans and machines, than comprehending a single sentence. However, the braiding of question and answer in a discourse, can sometimes help to reduce ambiguity.</p>
</div>
<div id="pragmatics" class="section level3 unnumbered hasAnchor">
<h3>Pragmatics<a href="text-mining-natural-language-processing.html#pragmatics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, pragmatics studies how context, world knowledge, language conventions, and other abstract properties contribute to the meaning of human conversation. Our shared experiences and knowledge often help us to make sense of situations. We derive meaning from the manner of the discourse, where it takes place, its time and length, who else is involved, and so forth. Thus, we usually find it much easier to communicate with those with whom we share a common culture, history, and socioeconomic status because the great collection of knowledge we jointly share assists in overcoming ambiguity.</p>
</div>
</div>
<div id="tokenization" class="section level2 unnumbered hasAnchor">
<h2>Tokenization<a href="text-mining-natural-language-processing.html#tokenization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tokenization is the process of breaking a document into chunks (e.g., words), which are called tokens. Whitespaces (e.g., spaces and tabs) are used to determine where a break occurs. Tokenization typically creates a <em>bag of words</em> for subsequent processing. Many text mining functions use words as the foundation for analysis.</p>
<div id="counting-words" class="section level3 unnumbered hasAnchor">
<h3>Counting words<a href="text-mining-natural-language-processing.html#counting-words" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To count the number of words in a string, simply count the number of times there are one or more consecutive spaces using the pattern “[:space:]+” and then add one, because the last word is not followed by a space.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="text-mining-natural-language-processing.html#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb350-2"><a href="text-mining-natural-language-processing.html#cb350-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str_count</span>(<span class="st">&quot;The dead batteries were given out free of charge&quot;</span>, <span class="st">&quot;[:space:]+&quot;</span>) <span class="sc">+</span> <span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 9</code></pre>
</div>
</div>
<div id="sentiment-analysis" class="section level2 unnumbered hasAnchor">
<h2>Sentiment analysis<a href="text-mining-natural-language-processing.html#sentiment-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sentiment analysis is a popular and simple method of measuring aggregate feeling. In its simplest form, it is computed by giving a score of +1 to each “positive” word and -1 to each “negative” word and summing the total to get a sentiment score. A text is decomposed into words. Each word is then checked against a list to find its score (i.e., +1 or -1), and if the word is not in the list, it doesn’t score.</p>
<p>A major shortcoming of sentiment analysis is that irony (e.g., “The name of Britain’s biggest dog (until it died) was Tiny”) and sarcasm (e.g., “I started out with nothing and still have most of it left”) are usually misclassified. Also, a phrase such as “not happy” might be scored as +1 by a sentiment analysis program that simply examines each word and not those around it.</p>
<p>The <strong>sentimentr package</strong> offers an advanced implementation of sentiment analysis. It is based on a polarity table, in which a word and its polarity score (e.g., -1 for a negative word) are recorded. The default polarity table is provided by the syuzhet package. You can create a polarity table suitable for your context, and you are not restricted to 1 or -1 for a word’s polarity score. Here are the first few rows of the default polarity table.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="text-mining-natural-language-processing.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb352-2"><a href="text-mining-natural-language-processing.html#cb352-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb352-3"><a href="text-mining-natural-language-processing.html#cb352-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">get_sentiment_dictionary</span>())</span></code></pre></div>
<pre><code>##          word value
## 1     abandon -0.75
## 2   abandoned -0.50
## 3   abandoner -0.25
## 4 abandonment -0.25
## 5    abandons -1.00
## 6    abducted -1.00</code></pre>
<p>In addition, sentimentr supports valence shifters, which are words that alter or intensify the meaning of a polarizing word (i.e., a word appearing in the polarity table) appearing in the text or document under examination. Each word has a value to indicate how to interpret its effect (negators (1), amplifiers(2), de-amplifiers (3), and conjunction (4).</p>
<p>Now, let’s see how we use the <strong>sentiment function</strong>. We’ll start with an example that does not use valence shifters, in which case we specify that the sentiment function should not look for valence words before or after any polarizing word. We indicate this by setting n.before and n.after to 0. Our sample text consists of several sentences, as shown in the following code.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="text-mining-natural-language-processing.html#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb354-2"><a href="text-mining-natural-language-processing.html#cb354-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb354-3"><a href="text-mining-natural-language-processing.html#cb354-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.</span></span>
<span id="cb354-4"><a href="text-mining-natural-language-processing.html#cb354-4" aria-hidden="true" tabindex="-1"></a><span class="st">So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your</span></span>
<span id="cb354-5"><a href="text-mining-natural-language-processing.html#cb354-5" aria-hidden="true" tabindex="-1"></a><span class="st">achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb354-6"><a href="text-mining-natural-language-processing.html#cb354-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before=</span><span class="dv">0</span>, <span class="at">n.after=</span><span class="dv">0</span>, <span class="at">amplifier.weight=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count  sentiment
## 1:          1           1          6  0.5511352
## 2:          2           1          6 -0.9185587
## 3:          2           2          2 -0.5303301
## 4:          2           3          1 -0.7500000
## 5:          3           1         12  0.6495191</code></pre>
<p>Notice that the sentiment function breaks each element (the text between quotes in this case) into sentences, identifies each sentence in an element, and computes the word count for each of these sentences. The sentiment score is the sum of the polarity scores divided by the square root of the number of words in the associated sentence.</p>
<p>To get the overall sentiment for the sample text, use:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="text-mining-natural-language-processing.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb356-2"><a href="text-mining-natural-language-processing.html#cb356-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb356-3"><a href="text-mining-natural-language-processing.html#cb356-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb356-4"><a href="text-mining-natural-language-processing.html#cb356-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sentiment</span>(sample,</span>
<span id="cb356-5"><a href="text-mining-natural-language-processing.html#cb356-5" aria-hidden="true" tabindex="-1"></a><span class="at">n.before=</span><span class="dv">0</span>, <span class="at">n.after=</span><span class="dv">0</span>)</span>
<span id="cb356-6"><a href="text-mining-natural-language-processing.html#cb356-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y<span class="sc">$</span>sentiment)</span></code></pre></div>
<pre><code>## [1] -0.1996469</code></pre>
<p>When a <strong>valence shift</strong> is detected before or after a polarizing word, its effect is incorporated in the sentiment calculation. The size of the effect is indicated by the amplifier.weight, a sentiment function parameter with a value between 0 and 1. The weight amplifies or de-amplifies by multiplying the polarized terms by 1 + the amplifier weight. A <strong><em>negator</em></strong> flips the sign of a polarizing word. A <strong><em>conjunction</em></strong> amplifies the current clause and down weights the prior clause. Some examples in the following table illustrate the results of applying the code to a variety of input text. The polarities are -1 (crazy) and 1 (love). There is a negator (not), two amplifiers (very and much), and a conjunction (but). Contractions are treated as amplifiers and so get weights based on the contraction (.9 in this case) and amplification (.8) in this case.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="text-mining-natural-language-processing.html#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb358-2"><a href="text-mining-natural-language-processing.html#cb358-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb358-3"><a href="text-mining-natural-language-processing.html#cb358-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.</span></span>
<span id="cb358-4"><a href="text-mining-natural-language-processing.html#cb358-4" aria-hidden="true" tabindex="-1"></a><span class="st">So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your</span></span>
<span id="cb358-5"><a href="text-mining-natural-language-processing.html#cb358-5" aria-hidden="true" tabindex="-1"></a><span class="st">achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb358-6"><a href="text-mining-natural-language-processing.html#cb358-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="dv">2</span>, <span class="at">n.after =</span> <span class="dv">2</span>, <span class="at">amplifier.weight=</span>.<span class="dv">8</span>, <span class="at">but.weight =</span> .<span class="dv">9</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count  sentiment
## 1:          1           1          6  0.5511352
## 2:          2           1          6 -0.9185587
## 3:          2           2          2 -0.5303301
## 4:          2           3          1 -0.7500000
## 5:          3           1         12  0.6495191</code></pre>
<table>
<colgroup>
<col width="19%" />
<col width="8%" />
<col width="56%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="center">Code</th>
<th align="left">Text</th>
<th align="right">Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="center"></td>
<td align="left">You’re crazy, and I love you.</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Negator</td>
<td align="center">1</td>
<td align="left">You’re not crazy, and I love you.</td>
<td align="right">0.57</td>
</tr>
<tr class="odd">
<td align="left">Amplifier</td>
<td align="center">2</td>
<td align="left">You’re crazy, and I love you very much.</td>
<td align="right">0.21</td>
</tr>
<tr class="even">
<td align="left">De-amplifier</td>
<td align="center">3</td>
<td align="left">You’re slightly crazy, and I love you.</td>
<td align="right">0.23</td>
</tr>
<tr class="odd">
<td align="left">Conjunction</td>
<td align="center">4</td>
<td align="left">You’re crazy, but I love you.</td>
<td align="right">0.45</td>
</tr>
</tbody>
</table>
<blockquote>
<p>❓ <em>Skill builder</em></p>
<p>Run the following R code and comment on how sensitive sentiment analysis is to the n.before and n.after parameters.</p>
</blockquote>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="text-mining-natural-language-processing.html#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb360-2"><a href="text-mining-natural-language-processing.html#cb360-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb360-3"><a href="text-mining-natural-language-processing.html#cb360-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re not crazy and I love you very much.&quot;</span>)</span>
<span id="cb360-4"><a href="text-mining-natural-language-processing.html#cb360-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="dv">4</span>, <span class="at">n.after=</span><span class="dv">3</span>, <span class="at">amplifier.weight=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          9   0.24975</code></pre>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="text-mining-natural-language-processing.html#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="cn">Inf</span>, <span class="at">n.after=</span><span class="cn">Inf</span>, <span class="at">amplifier.weight=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          9         0</code></pre>
<blockquote>
<p>What are the correct polarities for each word, and weights for negators, amplifiers and so on? It is a judgment call and one that is difficult to justify.</p>
</blockquote>
<p>Sentiment analysis has given you an idea of some of the issues surrounding text mining. Let’s now look at the topic in more depth and explore some of the tools available in <strong>tm</strong>, a general purpose text mining package for R. We will also use a few other R packages which support text mining and displaying the results.</p>
</div>
<div id="corpus" class="section level2 unnumbered hasAnchor">
<h2>Corpus<a href="text-mining-natural-language-processing.html#corpus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A collection of text is called a corpus. It is common to use N for the <em>corpus size</em>, the number of tokens, and V for the <em>vocabulary</em>, the number of distinct tokens.</p>
<p>In the following examples, our corpus consists of Warren Buffett’s annual letters for 1998-2012 to the <a href="http://www.berkshirehathaway.com/letters/letters.html">shareholders of Berkshire Hathaway</a> for 1998-2012. The original letters, available in html or pdf, were converted to <a href="http://www.terry.uga.edu/people/rwatson/">separate text files</a> using <a href="https://pdf.abbyy.com">Abbyy Fine Reader</a>. Tables, page numbers, and graphics were removed during the conversion.</p>
<p>The following R code sets up a loop to read each of the letters and add it to a data frame. When all the letters have been read, they are turned into a corpus.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="text-mining-natural-language-processing.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb364-2"><a href="text-mining-natural-language-processing.html#cb364-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb364-3"><a href="text-mining-natural-language-processing.html#cb364-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(NLP)</span>
<span id="cb364-4"><a href="text-mining-natural-language-processing.html#cb364-4" aria-hidden="true" tabindex="-1"></a><span class="co">#set up a data frame to hold up to 100 letters</span></span>
<span id="cb364-5"><a href="text-mining-natural-language-processing.html#cb364-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(<span class="at">num=</span><span class="dv">100</span>)</span>
<span id="cb364-6"><a href="text-mining-natural-language-processing.html#cb364-6" aria-hidden="true" tabindex="-1"></a>begin <span class="ot">&lt;-</span>  <span class="dv">1998</span> <span class="co"># date of first letter</span></span>
<span id="cb364-7"><a href="text-mining-natural-language-processing.html#cb364-7" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span>  begin</span>
<span id="cb364-8"><a href="text-mining-natural-language-processing.html#cb364-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read the letters</span></span>
<span id="cb364-9"><a href="text-mining-natural-language-processing.html#cb364-9" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (i <span class="sc">&lt;</span> <span class="dv">2013</span>) {</span>
<span id="cb364-10"><a href="text-mining-natural-language-processing.html#cb364-10" aria-hidden="true" tabindex="-1"></a> y <span class="ot">&lt;-</span> <span class="fu">as.character</span>(i)</span>
<span id="cb364-11"><a href="text-mining-natural-language-processing.html#cb364-11" aria-hidden="true" tabindex="-1"></a> <span class="co"># create the file name</span></span>
<span id="cb364-12"><a href="text-mining-natural-language-processing.html#cb364-12" aria-hidden="true" tabindex="-1"></a> f <span class="ot">&lt;-</span> <span class="fu">str_c</span>(<span class="st">&#39;http://www.richardtwatson.com/BuffettLetters/&#39;</span>,y, <span class="st">&#39;ltr.txt&#39;</span>,<span class="at">delim=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb364-13"><a href="text-mining-natural-language-processing.html#cb364-13" aria-hidden="true" tabindex="-1"></a> <span class="co"># read the letter as on large string</span></span>
<span id="cb364-14"><a href="text-mining-natural-language-processing.html#cb364-14" aria-hidden="true" tabindex="-1"></a> d <span class="ot">&lt;-</span>  <span class="fu">readChar</span>(f,<span class="at">nchars=</span><span class="fl">1e6</span>)</span>
<span id="cb364-15"><a href="text-mining-natural-language-processing.html#cb364-15" aria-hidden="true" tabindex="-1"></a> <span class="co"># add letter to the data frame</span></span>
<span id="cb364-16"><a href="text-mining-natural-language-processing.html#cb364-16" aria-hidden="true" tabindex="-1"></a> df[i<span class="sc">-</span>begin<span class="sc">+</span><span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span>  i</span>
<span id="cb364-17"><a href="text-mining-natural-language-processing.html#cb364-17" aria-hidden="true" tabindex="-1"></a> df[i<span class="sc">-</span>begin<span class="sc">+</span><span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span>  d</span>
<span id="cb364-18"><a href="text-mining-natural-language-processing.html#cb364-18" aria-hidden="true" tabindex="-1"></a> i <span class="ot">&lt;-</span>  i <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb364-19"><a href="text-mining-natural-language-processing.html#cb364-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb364-20"><a href="text-mining-natural-language-processing.html#cb364-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;doc_id&#39;</span>, <span class="st">&#39;text&#39;</span>)</span>
<span id="cb364-21"><a href="text-mining-natural-language-processing.html#cb364-21" aria-hidden="true" tabindex="-1"></a><span class="co"># create the corpus</span></span>
<span id="cb364-22"><a href="text-mining-natural-language-processing.html#cb364-22" aria-hidden="true" tabindex="-1"></a>letters <span class="ot">&lt;-</span>  <span class="fu">Corpus</span>(<span class="fu">DataframeSource</span>(df))</span></code></pre></div>
</div>
<div id="readability" class="section level2 unnumbered hasAnchor">
<h2>Readability<a href="text-mining-natural-language-processing.html#readability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are several approaches to estimating the readability of a selection of text. They are usually based on counting the words in each sentence and the number of syllables in each word. For example, the Flesch-Kincaid method uses the formula:</p>
<pre><code>(11.8 * syllables: per: word) + (0.39 * words:per:sentence) - 15.59</code></pre>
<p>It estimates the grade-level or years of education required of the reader. The bands are:</p>
<table>
<thead>
<tr class="header">
<th align="left">Score</th>
<th align="left">Education level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">13-16</td>
<td align="left">Undergraduate</td>
</tr>
<tr class="even">
<td align="left">16-18</td>
<td align="left">Masters</td>
</tr>
<tr class="odd">
<td align="left">19-</td>
<td align="left">PhD</td>
</tr>
</tbody>
</table>
<p>The R package koRpus has a number of methods for calculating readability scores. You first need to tokenize the text using the package’s tokenize function. Then complete the calculation.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="text-mining-natural-language-processing.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(koRpus)</span>
<span id="cb366-2"><a href="text-mining-natural-language-processing.html#cb366-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(koRpus.lang.en)</span>
<span id="cb366-3"><a href="text-mining-natural-language-processing.html#cb366-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sylly)</span>
<span id="cb366-4"><a href="text-mining-natural-language-processing.html#cb366-4" aria-hidden="true" tabindex="-1"></a><span class="co">#tokenize the first letter in the corpus after converting to character vector</span></span>
<span id="cb366-5"><a href="text-mining-natural-language-processing.html#cb366-5" aria-hidden="true" tabindex="-1"></a>txt <span class="ot">&lt;-</span>  letters[[<span class="dv">1</span>]][<span class="dv">1</span>] <span class="co"># first element in the list</span></span>
<span id="cb366-6"><a href="text-mining-natural-language-processing.html#cb366-6" aria-hidden="true" tabindex="-1"></a>tagged.text <span class="ot">&lt;-</span> <span class="fu">tokenize</span>(<span class="fu">as.character</span>(txt),<span class="at">format=</span><span class="st">&quot;obj&quot;</span>,<span class="at">lang=</span><span class="st">&quot;en&quot;</span>)</span>
<span id="cb366-7"><a href="text-mining-natural-language-processing.html#cb366-7" aria-hidden="true" tabindex="-1"></a><span class="co"># score</span></span>
<span id="cb366-8"><a href="text-mining-natural-language-processing.html#cb366-8" aria-hidden="true" tabindex="-1"></a><span class="fu">readability</span>(tagged.text, <span class="at">hyphen=</span><span class="cn">NULL</span>,<span class="at">index=</span><span class="st">&quot;FORCAST&quot;</span>, <span class="at">quiet =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## FORCAST
##   Parameters: default 
##        Grade: 9.89 
##          Age: 14.89 
## 
## Text language: en</code></pre>
</div>
<div id="preprocessing" class="section level2 unnumbered hasAnchor">
<h2>Preprocessing<a href="text-mining-natural-language-processing.html#preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before commencing analysis, a text file typically needs to be prepared for processing. Several steps are usually taken.</p>
<div id="case-conversion" class="section level3 unnumbered hasAnchor">
<h3>Case conversion<a href="text-mining-natural-language-processing.html#case-conversion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For comparison purposes, all text should be of the same case. Conventionally, the choice is to convert to all lower case. First convert to UTF-8.</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="text-mining-natural-language-processing.html#cb368-1" aria-hidden="true" tabindex="-1"></a>content_transformer <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">iconv</span>(x, <span class="at">to=</span><span class="st">&#39;UTF-8-MAC&#39;</span>, <span class="at">sub=</span><span class="st">&#39;byte&#39;</span>)</span>
<span id="cb368-2"><a href="text-mining-natural-language-processing.html#cb368-2" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(letters, content_transformer)</span>
<span id="cb368-3"><a href="text-mining-natural-language-processing.html#cb368-3" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(letters,tolower)</span></code></pre></div>
</div>
<div id="punctuation-removal" class="section level3 unnumbered hasAnchor">
<h3>Punctuation removal<a href="text-mining-natural-language-processing.html#punctuation-removal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Punctuation is usually removed when the focus is just on the words in a text and not on higher level elements such as sentences and paragraphs.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="text-mining-natural-language-processing.html#cb369-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removePunctuation)</span></code></pre></div>
</div>
<div id="number-removal" class="section level3 unnumbered hasAnchor">
<h3>Number removal<a href="text-mining-natural-language-processing.html#number-removal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You might also want to remove all numbers.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="text-mining-natural-language-processing.html#cb370-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeNumbers)</span></code></pre></div>
</div>
<div id="stripping-extra-white-spaces" class="section level3 unnumbered hasAnchor">
<h3>Stripping extra white spaces<a href="text-mining-natural-language-processing.html#stripping-extra-white-spaces" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Removing extra spaces, tabs, and such is another common preprocessing action.</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="text-mining-natural-language-processing.html#cb371-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,stripWhitespace)</span></code></pre></div>
<blockquote>
<p>❓ <em>Skill builder</em></p>
<p>Redo the readability calculation after executing the preprocessing steps described in the previous section. What do you observe?</p>
</blockquote>
</div>
<div id="stop-word-filtering" class="section level3 unnumbered hasAnchor">
<h3>Stop word filtering<a href="text-mining-natural-language-processing.html#stop-word-filtering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Stop words are short common words that can be removed from a text without affecting the results of an analysis. Though there is no commonly agreed upon list of stop works, typically included are <em>the, is, be, and, but, to,</em> and <em>on</em>. Stop word lists are typically all lowercase, thus you should convert to lowercase before removing stop words. Each language has a set of stop words. In the following sample code, we use the <a href="http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop">SMART</a> list of English stop words.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="text-mining-natural-language-processing.html#cb372-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeWords,<span class="fu">stopwords</span>(<span class="st">&quot;SMART&quot;</span>))</span></code></pre></div>
</div>
<div id="specific-word-removal" class="section level3 unnumbered hasAnchor">
<h3>Specific word removal<a href="text-mining-natural-language-processing.html#specific-word-removal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There can also specify particular words to be removed via a character vector. For instance, you might not be interested in tracking references to <em>Berkshire Hathaway</em> in Buffett’s letters. You can set up a dictionary with words to be removed from the corpus.</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="text-mining-natural-language-processing.html#cb373-1" aria-hidden="true" tabindex="-1"></a>dictionary <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;berkshire&quot;</span>,<span class="st">&quot;hathaway&quot;</span>, <span class="st">&quot;million&quot;</span>, <span class="st">&quot;billion&quot;</span>, <span class="st">&quot;dollar&quot;</span>)</span>
<span id="cb373-2"><a href="text-mining-natural-language-processing.html#cb373-2" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeWords,dictionary)</span></code></pre></div>
</div>
<div id="word-length-filtering" class="section level3 unnumbered hasAnchor">
<h3>Word length filtering<a href="text-mining-natural-language-processing.html#word-length-filtering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can also apply a filter to remove all words less than or greater than a specified lengths. The tm package provides this option when generating a term frequency matrix, something you will read about shortly.</p>
</div>
<div id="parts-of-speech-pos-filtering" class="section level3 unnumbered hasAnchor">
<h3>Parts of speech (POS) filtering<a href="text-mining-natural-language-processing.html#parts-of-speech-pos-filtering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another option is to remove particular types of words. For example, you might scrub all adverbs and adjectives.</p>
</div>
<div id="stemming" class="section level3 unnumbered hasAnchor">
<h3>Stemming<a href="text-mining-natural-language-processing.html#stemming" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Stemming reduces inflected or derived words to their stem or root form. For example, <em>cats</em> and <em>catty</em> stem to <em>cat</em>. <em>Fishlike</em> and <em>fishy</em> stem to <em>fish</em>. As a stemmer generally works by suffix stripping, so <em>catfish</em> should stem to <em>cat</em>. As you would expect, stemmers are available for different languages, and thus the language must be specified. Stemming can take a while to process.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="text-mining-natural-language-processing.html#cb374-1" aria-hidden="true" tabindex="-1"></a>stem.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,stemDocument, <span class="at">language =</span></span>
<span id="cb374-2"><a href="text-mining-natural-language-processing.html#cb374-2" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;english&quot;</span>)</span></code></pre></div>
<p>Following stemming, you can apply <strong>stem completion</strong> to return stems to their original form to make the text more readable. The original document that was stemmed, in this case, is used as the dictionary to search for possible completions. Stem completion can apply several different rules for converting a stem to a word, including “prevalent” for the most frequent match, “first” for the first found completion, and “longest” and “shortest” for the longest and shortest, respectively, completion in terms of characters</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="text-mining-natural-language-processing.html#cb375-1" aria-hidden="true" tabindex="-1"></a>stem.letters <span class="ot">&lt;-</span>  <span class="fu">tm_map</span>(clean.letters,stemDocument, <span class="at">language =</span> <span class="st">&quot;english&quot;</span>)</span></code></pre></div>
</div>
<div id="regex-filtering" class="section level3 unnumbered hasAnchor">
<h3>Regex filtering<a href="text-mining-natural-language-processing.html#regex-filtering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The power of regex (regular expressions) can also be used for filtering text or searching and replacing text. You might recall that we covered regex when learning SQL.</p>
</div>
</div>
<div id="word-frequency-analysis" class="section level2 unnumbered hasAnchor">
<h2>Word frequency analysis<a href="text-mining-natural-language-processing.html#word-frequency-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Word frequency analysis is a simple technique that can also be the foundation for other analyses. The method is based on creating a matrix in one of two forms.</p>
<p>A <strong>term-document matrix</strong> contains one row for each term and one column for each document.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="text-mining-natural-language-processing.html#cb376-1" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb376-2"><a href="text-mining-natural-language-processing.html#cb376-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tdm)</span></code></pre></div>
<pre><code>## [1] 6966   15</code></pre>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="text-mining-natural-language-processing.html#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="co"># report those words occurring more than 100 times</span></span>
<span id="cb378-2"><a href="text-mining-natural-language-processing.html#cb378-2" aria-hidden="true" tabindex="-1"></a><span class="fu">findFreqTerms</span>(tdm, <span class="at">lowfreq =</span> <span class="dv">100</span>, <span class="at">highfreq =</span> <span class="cn">Inf</span>)</span></code></pre></div>
<pre><code>##   [1] &quot;account&quot;     &quot;acquisit&quot;    &quot;addit&quot;       &quot;ago&quot;         &quot;american&quot;   
##   [6] &quot;amount&quot;      &quot;annual&quot;      &quot;asset&quot;       &quot;berkshir&quot;    &quot;bond&quot;       
##  [11] &quot;book&quot;        &quot;busi&quot;        &quot;buy&quot;         &quot;call&quot;        &quot;capit&quot;      
##  [16] &quot;case&quot;        &quot;cash&quot;        &quot;ceo&quot;         &quot;charg&quot;       &quot;charli&quot;     
##  [21] &quot;compani&quot;     &quot;continu&quot;     &quot;corpor&quot;      &quot;cost&quot;        &quot;countri&quot;    
##  [26] &quot;custom&quot;      &quot;day&quot;         &quot;deliv&quot;       &quot;director&quot;    &quot;earn&quot;       
##  [31] &quot;econom&quot;      &quot;expect&quot;      &quot;expens&quot;      &quot;famili&quot;      &quot;financi&quot;    
##  [36] &quot;float&quot;       &quot;fund&quot;        &quot;futur&quot;       &quot;gain&quot;        &quot;geico&quot;      
##  [41] &quot;general&quot;     &quot;give&quot;        &quot;good&quot;        &quot;great&quot;       &quot;group&quot;      
##  [46] &quot;growth&quot;      &quot;hold&quot;        &quot;huge&quot;        &quot;import&quot;      &quot;includ&quot;     
##  [51] &quot;incom&quot;       &quot;increas&quot;     &quot;industri&quot;    &quot;insur&quot;       &quot;interest&quot;   
##  [56] &quot;intrins&quot;     &quot;invest&quot;      &quot;investor&quot;    &quot;job&quot;         &quot;larg&quot;       
##  [61] &quot;largest&quot;     &quot;long&quot;        &quot;loss&quot;        &quot;made&quot;        &quot;major&quot;      
##  [66] &quot;make&quot;        &quot;manag&quot;       &quot;market&quot;      &quot;meet&quot;        &quot;money&quot;      
##  [71] &quot;net&quot;         &quot;number&quot;      &quot;offer&quot;       &quot;open&quot;        &quot;oper&quot;       
##  [76] &quot;own&quot;         &quot;owner&quot;       &quot;page&quot;        &quot;paid&quot;        &quot;pay&quot;        
##  [81] &quot;peopl&quot;       &quot;perform&quot;     &quot;period&quot;      &quot;point&quot;       &quot;polici&quot;     
##  [86] &quot;posit&quot;       &quot;premium&quot;     &quot;present&quot;     &quot;price&quot;       &quot;problem&quot;    
##  [91] &quot;produc&quot;      &quot;product&quot;     &quot;profit&quot;      &quot;purchas&quot;     &quot;put&quot;        
##  [96] &quot;question&quot;    &quot;rate&quot;        &quot;receiv&quot;      &quot;record&quot;      &quot;reinsur&quot;    
## [101] &quot;remain&quot;      &quot;report&quot;      &quot;requir&quot;      &quot;reserv&quot;      &quot;result&quot;     
## [106] &quot;return&quot;      &quot;run&quot;         &quot;sale&quot;        &quot;saturday&quot;    &quot;sell&quot;       
## [111] &quot;servic&quot;      &quot;share&quot;       &quot;sharehold&quot;   &quot;signific&quot;    &quot;special&quot;    
## [116] &quot;stock&quot;       &quot;sunday&quot;      &quot;tax&quot;         &quot;time&quot;        &quot;today&quot;      
## [121] &quot;total&quot;       &quot;underwrit&quot;   &quot;work&quot;        &quot;world&quot;       &quot;worth&quot;      
## [126] &quot;year&quot;        &quot;—&quot;           &quot;contract&quot;    &quot;home&quot;        &quot;midamerican&quot;
## [131] &quot;risk&quot;        &quot;deriv&quot;       &quot;clayton&quot;</code></pre>
<p>A <strong>document-term matrix</strong> contains one row for each document and one column for each term.</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="text-mining-natural-language-processing.html#cb380-1" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> <span class="fu">DocumentTermMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb380-2"><a href="text-mining-natural-language-processing.html#cb380-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(dtm)</span></code></pre></div>
<pre><code>## [1]   15 6966</code></pre>
<p>The function dtm() reports the number of distinct terms, the vocabulary, and the number of documents in the corpus.</p>
<div id="term-frequency" class="section level3 unnumbered hasAnchor">
<h3>Term frequency<a href="text-mining-natural-language-processing.html#term-frequency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Words that occur frequently within a document are usually a good indicator of the document’s content. Term frequency (tf) measures word frequency.</p>
<p>tf<sub>td</sub> = number of times term t occurs in document d.</p>
<p>Here is the R code for determining the frequency of words in a corpus.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="text-mining-natural-language-processing.html#cb382-1" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb382-2"><a href="text-mining-natural-language-processing.html#cb382-2" aria-hidden="true" tabindex="-1"></a><span class="co"># convert term document matrix to a regular matrix to get frequencies of words</span></span>
<span id="cb382-3"><a href="text-mining-natural-language-processing.html#cb382-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span>  <span class="fu">as.matrix</span>(tdm)</span>
<span id="cb382-4"><a href="text-mining-natural-language-processing.html#cb382-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sort on frequency of terms</span></span>
<span id="cb382-5"><a href="text-mining-natural-language-processing.html#cb382-5" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">rowSums</span>(m), <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb382-6"><a href="text-mining-natural-language-processing.html#cb382-6" aria-hidden="true" tabindex="-1"></a><span class="co"># display the ten most frequent words</span></span>
<span id="cb382-7"><a href="text-mining-natural-language-processing.html#cb382-7" aria-hidden="true" tabindex="-1"></a>v[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##      year      busi   compani      earn      oper     insur     manag    invest 
##      1288       969       709       665       555       547       476       405 
##      make sharehold 
##       353       348</code></pre>
<p>A probability density plot shows the distribution of words in a document visually. As you can see, there is a very long and thin tail because a very small number of words occur frequently. Note that this plot shows the distribution of words after the removal of stop words.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="text-mining-natural-language-processing.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb384-2"><a href="text-mining-natural-language-processing.html#cb384-2" aria-hidden="true" tabindex="-1"></a><span class="co"># get the names corresponding to the words</span></span>
<span id="cb384-3"><a href="text-mining-natural-language-processing.html#cb384-3" aria-hidden="true" tabindex="-1"></a>names <span class="ot">&lt;-</span> <span class="fu">names</span>(v)</span>
<span id="cb384-4"><a href="text-mining-natural-language-processing.html#cb384-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create a data frame for plotting</span></span>
<span id="cb384-5"><a href="text-mining-natural-language-processing.html#cb384-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">word=</span>names, <span class="at">freq=</span>v)</span>
<span id="cb384-6"><a href="text-mining-natural-language-processing.html#cb384-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d,<span class="fu">aes</span>(freq)) <span class="sc">+</span></span>
<span id="cb384-7"><a href="text-mining-natural-language-processing.html#cb384-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill=</span><span class="st">&quot;salmon&quot;</span>) <span class="sc">+</span></span>
<span id="cb384-8"><a href="text-mining-natural-language-processing.html#cb384-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span></span>
<span id="cb384-9"><a href="text-mining-natural-language-processing.html#cb384-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:density-plot"></span>
<img src="DataManagement_files/figure-html/density-plot-1.png" alt="Probability density plot of word frequency" width="672" />
<p class="caption">
Figure 17.1: Probability density plot of word frequency
</p>
</div>
<p>A word cloud is way of visualizing the most frequent words.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="text-mining-natural-language-processing.html#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wordcloud)</span>
<span id="cb385-2"><a href="text-mining-natural-language-processing.html#cb385-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb385-3"><a href="text-mining-natural-language-processing.html#cb385-3" aria-hidden="true" tabindex="-1"></a><span class="co"># select the color palette</span></span>
<span id="cb385-4"><a href="text-mining-natural-language-processing.html#cb385-4" aria-hidden="true" tabindex="-1"></a>pal <span class="ot">=</span> <span class="fu">brewer.pal</span>(<span class="dv">5</span>,<span class="st">&quot;Accent&quot;</span>)</span>
<span id="cb385-5"><a href="text-mining-natural-language-processing.html#cb385-5" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the cloud based on the 30 most frequent words</span></span>
<span id="cb385-6"><a href="text-mining-natural-language-processing.html#cb385-6" aria-hidden="true" tabindex="-1"></a><span class="fu">wordcloud</span>(d<span class="sc">$</span>word, d<span class="sc">$</span>freq, <span class="at">min.freq=</span>d<span class="sc">$</span>freq[<span class="dv">30</span>],<span class="at">colors=</span>pal)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cloud"></span>
<img src="DataManagement_files/figure-html/cloud-1.png" alt="A word cloud" width="672" />
<p class="caption">
Figure 17.2: A word cloud
</p>
</div>
<blockquote>
<p>❓ <strong>Skill builder</strong></p>
<p>Start with the original letters corpus (i.e., prior to preprocessing) and identify the 20 most common words and create a word cloud for these words.</p>
</blockquote>
</div>
</div>
<div id="co-occurrence-and-association" class="section level2 unnumbered hasAnchor">
<h2>Co-occurrence and association<a href="text-mining-natural-language-processing.html#co-occurrence-and-association" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Co-occurrence measures the frequency with which two words appear together. In the case of document level association, if the two words both appear or neither appears, then the correlation or association is 1. If two words never appear together in the same document, their association is -1.</p>
<p>A simple example illustrates the concept. The following code sets up a corpus of five elementary documents.</p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="text-mining-natural-language-processing.html#cb386-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb386-2"><a href="text-mining-natural-language-processing.html#cb386-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span>  <span class="fu">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word1 word2&quot;</span>,<span class="st">&quot;word1 word2 word3&quot;</span>,<span class="st">&quot;word1 word2 word3 word4&quot;</span>,<span class="st">&quot;word1 word2 word3 word4 word5&quot;</span>)</span>
<span id="cb386-3"><a href="text-mining-natural-language-processing.html#cb386-3" aria-hidden="true" tabindex="-1"></a>corpus <span class="ot">&lt;-</span>  <span class="fu">VCorpus</span>(<span class="fu">VectorSource</span>(data))</span>
<span id="cb386-4"><a href="text-mining-natural-language-processing.html#cb386-4" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(corpus)</span>
<span id="cb386-5"><a href="text-mining-natural-language-processing.html#cb386-5" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(tdm)</span></code></pre></div>
<pre><code>##        Docs
## Terms   1 2 3 4 5
##   word1 1 1 1 1 1
##   word2 0 1 1 1 1
##   word3 0 0 1 1 1
##   word4 0 0 0 1 1
##   word5 0 0 0 0 1</code></pre>
<p>We compute the correlation of rows to get a measure of association across documents.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="text-mining-natural-language-processing.html#cb388-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation between word2 and word3, word4, and word5</span></span>
<span id="cb388-2"><a href="text-mining-natural-language-processing.html#cb388-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.6123724</code></pre>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="text-mining-natural-language-processing.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.4082483</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="text-mining-natural-language-processing.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>Alternatively, use the findAssocs function, which computes all correlations between a given term and all terms in the term-document matrix and reports those higher than the correlation threshold.</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="text-mining-natural-language-processing.html#cb394-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb394-2"><a href="text-mining-natural-language-processing.html#cb394-2" aria-hidden="true" tabindex="-1"></a><span class="co"># find associations greater than 0.1</span></span>
<span id="cb394-3"><a href="text-mining-natural-language-processing.html#cb394-3" aria-hidden="true" tabindex="-1"></a><span class="fu">findAssocs</span>(tdm,<span class="st">&quot;word2&quot;</span>,<span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>## $word2
## word3 word4 word5 
##  0.61  0.41  0.25</code></pre>
<p>Now that you have an understanding of how association works across documents, here is an example for the corpus of Buffett letters.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="text-mining-natural-language-processing.html#cb396-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the first ten letters</span></span>
<span id="cb396-2"><a href="text-mining-natural-language-processing.html#cb396-2" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span> <span class="fu">TermDocumentMatrix</span>(stem.letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb396-3"><a href="text-mining-natural-language-processing.html#cb396-3" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the associations</span></span>
<span id="cb396-4"><a href="text-mining-natural-language-processing.html#cb396-4" aria-hidden="true" tabindex="-1"></a><span class="fu">findAssocs</span>(tdm, <span class="st">&quot;invest&quot;</span>,<span class="fl">0.80</span>)</span></code></pre></div>
<pre><code>## $invest
##      bare     earth imperfect     resum     susan 
##      0.88      0.88      0.88      0.84      0.84</code></pre>
</div>
<div id="cluster-analysis" class="section level2 unnumbered hasAnchor">
<h2>Cluster analysis<a href="text-mining-natural-language-processing.html#cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cluster analysis is a statistical technique for grouping together sets of observations that share common characteristics. Objects assigned to the same group are more similar in some way than those allocated to another cluster. In the case of a corpus, cluster analysis groups documents based on their similarity. Google, for instance, uses clustering for its news site.</p>
<p>The general principle of cluster analysis is to map a set of observations in multidimensional space. For example, if you have seven measures for each observation, each will be mapped into seven-dimensional space. Observations that are close together in this space will be grouped together. In the case of a corpus, cluster analysis is based on mapping frequently occurring words into a multidimensional space. The frequency with which each word appears in a document is used as a weight, so that frequently occurring words have more influence than others.</p>
<p>There are multiple statistical techniques for clustering, and multiple methods for calculating the distance between points. Furthermore, the analyst has to decide how many clusters to create. Thus, cluster analysis requires some judgment and experimentation to develop a meaningful set of groups.</p>
<p>The following code computes all possible clusters using the Ward method of cluster analysis. A term-document matrix is sparse, which means it consists mainly of zeroes. In other words, many terms occur in only one or two documents, and the cell entries for the remaining documents are zero. In order to reduce the computations required, sparse terms are removed from the matrix. You can vary the sparse parameter to see how the clusters vary.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="text-mining-natural-language-processing.html#cb398-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster analysis</span></span>
<span id="cb398-2"><a href="text-mining-natural-language-processing.html#cb398-2" aria-hidden="true" tabindex="-1"></a><span class="co"># name the columns for the letter&#39;s year</span></span>
<span id="cb398-3"><a href="text-mining-natural-language-processing.html#cb398-3" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span> <span class="fu">TermDocumentMatrix</span>(stem.letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>])</span>
<span id="cb398-4"><a href="text-mining-natural-language-processing.html#cb398-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(tdm) <span class="ot">&lt;-</span>  <span class="dv">1998</span><span class="sc">:</span><span class="dv">2012</span></span>
<span id="cb398-5"><a href="text-mining-natural-language-processing.html#cb398-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove sparse terms</span></span>
<span id="cb398-6"><a href="text-mining-natural-language-processing.html#cb398-6" aria-hidden="true" tabindex="-1"></a>tdm1 <span class="ot">&lt;-</span> <span class="fu">removeSparseTerms</span>(tdm, <span class="fl">0.5</span>)</span>
<span id="cb398-7"><a href="text-mining-natural-language-processing.html#cb398-7" aria-hidden="true" tabindex="-1"></a><span class="co"># transpose the matrix</span></span>
<span id="cb398-8"><a href="text-mining-natural-language-processing.html#cb398-8" aria-hidden="true" tabindex="-1"></a>tdmtranspose <span class="ot">&lt;-</span>  <span class="fu">t</span>(tdm1)</span>
<span id="cb398-9"><a href="text-mining-natural-language-processing.html#cb398-9" aria-hidden="true" tabindex="-1"></a>cluster <span class="ot">=</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(tdmtranspose))</span>
<span id="cb398-10"><a href="text-mining-natural-language-processing.html#cb398-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the tree</span></span>
<span id="cb398-11"><a href="text-mining-natural-language-processing.html#cb398-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cluster)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cluster"></span>
<img src="DataManagement_files/figure-html/cluster-1.png" alt="Dendrogram for Buffett letters from 1998-2012" width="672" />
<p class="caption">
Figure 17.3: Dendrogram for Buffett letters from 1998-2012
</p>
</div>
<p>The cluster analysis is shown in the following figure as a dendrogram, a tree diagram, with a leaf for each year. Clusters seem to from around consecutive years. Can you think of an explanation?</p>
</div>
<div id="topic-modeling" class="section level2 unnumbered hasAnchor">
<h2>Topic modeling<a href="text-mining-natural-language-processing.html#topic-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Topic modeling is a set of statistical techniques for identifying the themes that occur in a document set. The key assumption is that a document on a particular topic will contain words that identify that topic. For example, a report on gold mining will likely contain words such as “gold” and “ore.” Whereas, a document on France, would likely contain the terms “France,” “French,” and “Paris.”</p>
<p>The package <strong>topicmodels</strong> implements topic modeling techniques within the R framework. It extends tm to provide support for topic modeling. It implements two methods: latent Dirichlet allocation (LDA), which assumes topics are uncorrelated; and correlated topic models (CTM), an extension of LDA that permits correlation between topics.<a href="data-administration.html#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> Both LDA and CTM require that the number of topics to be extracted is determined a priori. For example, you might decide in advance that five topics gives a reasonable spread and is sufficiently small for the diversity to be understood.<a href="data-administration.html#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<p>Words that occur frequently in many documents are not good at distinguishing among documents. The weighted term frequency inverse document frequency (tf-idf) is a measure designed for determining which terms discriminate among documents. It is based on the term frequency (tf), defined earlier, and the inverse document frequency.</p>
<div id="inverse-document-frequency" class="section level3 unnumbered hasAnchor">
<h3>Inverse document frequency<a href="text-mining-natural-language-processing.html#inverse-document-frequency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The inverse document frequency (idf) measures the frequency of a term across documents.</p>
<p><img src="Figures/Chapter%2017/idf.png" /></p>
<blockquote>
<p>Where m = number of documents (i.e., rows in the case of a term-document matrix); df<sub>t</sub> = number of documents containing term <em>t.</em></p>
</blockquote>
<p>If a term occurs in every document, then its idf = 0, whereas if a term occurs in only one document out of 15, its idf = 3.91.</p>
<p>To calculate and display the idf for the letters corpus, we can use the following R script.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="text-mining-natural-language-processing.html#cb399-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate idf for each term</span></span>
<span id="cb399-2"><a href="text-mining-natural-language-processing.html#cb399-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(slam)</span>
<span id="cb399-3"><a href="text-mining-natural-language-processing.html#cb399-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb399-4"><a href="text-mining-natural-language-processing.html#cb399-4" aria-hidden="true" tabindex="-1"></a>idf <span class="ot">&lt;-</span>  <span class="fu">log2</span>(<span class="fu">nDocs</span>(dtm)<span class="sc">/</span><span class="fu">col_sums</span>(dtm <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb399-5"><a href="text-mining-natural-language-processing.html#cb399-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataframe for graphing</span></span>
<span id="cb399-6"><a href="text-mining-natural-language-processing.html#cb399-6" aria-hidden="true" tabindex="-1"></a>df.idf <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(idf)</span>
<span id="cb399-7"><a href="text-mining-natural-language-processing.html#cb399-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.idf,<span class="fu">aes</span>(idf)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;chocolate&quot;</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Inverse document frequency&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:topic-1"></span>
<img src="DataManagement_files/figure-html/topic-1-1.png" alt="Inverse document frequency (corpus has 15 documents)" width="672" />
<p class="caption">
Figure 17.4: Inverse document frequency (corpus has 15 documents)
</p>
</div>
<p>The generated graphic shows that about 5,000 terms occur in only one document (i.e., the idf = 3.91) and less than 500 terms occur in every document. The terms with an idf in the range 1 to 2 are likely to be the most useful in determining the topic of each document.</p>
</div>
<div id="term-frequency-inverse-document-frequency-tf-idf" class="section level3 unnumbered hasAnchor">
<h3>Term frequency inverse document frequency (tf-idf)<a href="text-mining-natural-language-processing.html#term-frequency-inverse-document-frequency-tf-idf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The weighted term frequency inverse document frequency (tf-idf or ω<sub>td</sub>) is calculated by multiplying a term’s frequency (tf) by its inverse document frequency (idf).</p>
<p><img src="Figures/Chapter%2017/tf-idf.png" /></p>
<blockquote>
<p>Where tf<sub>td</sub> = frequency of term <em>t</em> in document <em>d.</em></p>
</blockquote>
</div>
<div id="topic-modeling-with-r" class="section level3 unnumbered hasAnchor">
<h3>Topic modeling with R<a href="text-mining-natural-language-processing.html#topic-modeling-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Prior to topic modeling, pre-process a text file in the usual fashion (e.g., convert to lower case, remove punctuation, and so forth). Then, create a document-term matrix.</p>
<p>The mean term frequency-inverse document frequency (tf-idf) is used to select the vocabulary for topic modeling. We use the median value of tf-idf for all terms as a threshold.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="text-mining-natural-language-processing.html#cb400-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb400-2"><a href="text-mining-natural-language-processing.html#cb400-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(slam)</span>
<span id="cb400-3"><a href="text-mining-natural-language-processing.html#cb400-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate tf-idf for each term</span></span>
<span id="cb400-4"><a href="text-mining-natural-language-processing.html#cb400-4" aria-hidden="true" tabindex="-1"></a>tfidf <span class="ot">&lt;-</span>  <span class="fu">tapply</span>(dtm<span class="sc">$</span>v<span class="sc">/</span><span class="fu">row_sums</span>(dtm)[dtm<span class="sc">$</span>i], dtm<span class="sc">$</span>j, mean) <span class="sc">*</span> <span class="fu">log2</span>(<span class="fu">nDocs</span>(dtm)<span class="sc">/</span><span class="fu">col_sums</span>(dtm <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb400-5"><a href="text-mining-natural-language-processing.html#cb400-5" aria-hidden="true" tabindex="-1"></a><span class="co"># report dimensions (terms)</span></span>
<span id="cb400-6"><a href="text-mining-natural-language-processing.html#cb400-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tfidf)</span></code></pre></div>
<pre><code>## [1] 6966</code></pre>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="text-mining-natural-language-processing.html#cb402-1" aria-hidden="true" tabindex="-1"></a><span class="co"># report median to use as cut-off point</span></span>
<span id="cb402-2"><a href="text-mining-natural-language-processing.html#cb402-2" aria-hidden="true" tabindex="-1"></a><span class="fu">median</span>(tfidf)</span></code></pre></div>
<pre><code>## [1] 0.0006601708</code></pre>
<p>The goal of topic modeling is to find those terms that distinguish a document set. Thus, terms with low frequency should be omitted because they don’t occur often enough to define a topic. Similarly, those terms occurring in many documents don’t differentiate between documents.</p>
<p>A common heuristic is to select those terms with a tf-idf &gt; median(tf-idf). As a result, we reduce the document-term matrix by keeping only those terms above the threshold and eliminating rows that have zero terms. Because the median is a measure of central tendency, this approach reduces the number of columns by roughly half.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="text-mining-natural-language-processing.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select columns with tf-idf &gt; median</span></span>
<span id="cb404-2"><a href="text-mining-natural-language-processing.html#cb404-2" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> dtm[,tfidf <span class="sc">&gt;=</span> <span class="fu">median</span>(tfidf)]</span>
<span id="cb404-3"><a href="text-mining-natural-language-processing.html#cb404-3" aria-hidden="true" tabindex="-1"></a><span class="co">#select rows with rowsum &gt; 0</span></span>
<span id="cb404-4"><a href="text-mining-natural-language-processing.html#cb404-4" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> dtm[<span class="fu">row_sums</span>(dtm) <span class="sc">&gt;</span> <span class="dv">0</span>,]</span>
<span id="cb404-5"><a href="text-mining-natural-language-processing.html#cb404-5" aria-hidden="true" tabindex="-1"></a><span class="co"># report reduced dimension</span></span>
<span id="cb404-6"><a href="text-mining-natural-language-processing.html#cb404-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(dtm)</span></code></pre></div>
<pre><code>## [1]   15 3509</code></pre>
<p>As mentioned earlier, the topic modeling method assumes a set number of topics, and it is the responsibility of the analyst to estimate the correct number of topics to extract. It is common practice to fit models with a varying number of topics, and use the various results to establish a good choice for the number of topics. The analyst will typically review the output of several models and make a judgment on which model appears to provide a realistic set of distinct topics. Here is some code that starts with five topics.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="text-mining-natural-language-processing.html#cb406-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set number of topics to extract</span></span>
<span id="cb406-2"><a href="text-mining-natural-language-processing.html#cb406-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># number of topics</span></span>
<span id="cb406-3"><a href="text-mining-natural-language-processing.html#cb406-3" aria-hidden="true" tabindex="-1"></a>SEED <span class="ot">&lt;-</span> <span class="dv">2010</span> <span class="co"># seed for initializing the model rather than the default random</span></span>
<span id="cb406-4"><a href="text-mining-natural-language-processing.html#cb406-4" aria-hidden="true" tabindex="-1"></a><span class="co"># try multiple methods – takes a while for a big corpus</span></span>
<span id="cb406-5"><a href="text-mining-natural-language-processing.html#cb406-5" aria-hidden="true" tabindex="-1"></a>TM <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">VEM =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED)),</span>
<span id="cb406-6"><a href="text-mining-natural-language-processing.html#cb406-6" aria-hidden="true" tabindex="-1"></a><span class="at">VEM_fixed =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">estimate.alpha =</span> <span class="cn">FALSE</span>, <span class="at">seed =</span> SEED)),</span>
<span id="cb406-7"><a href="text-mining-natural-language-processing.html#cb406-7" aria-hidden="true" tabindex="-1"></a><span class="at">Gibbs =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">method =</span> <span class="st">&quot;Gibbs&quot;</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED, <span class="at">burnin =</span> <span class="dv">1000</span>,  <span class="at">thin =</span> <span class="dv">100</span>, <span class="at">iter =</span> <span class="dv">1000</span>)), <span class="at">CTM =</span> <span class="fu">CTM</span>(dtm, <span class="at">k =</span> k,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED, <span class="at">var =</span> <span class="fu">list</span>(<span class="at">tol =</span> <span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span>), <span class="at">em =</span> <span class="fu">list</span>(<span class="at">tol =</span> <span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span>))))</span>
<span id="cb406-8"><a href="text-mining-natural-language-processing.html#cb406-8" aria-hidden="true" tabindex="-1"></a><span class="fu">topics</span>(TM[[<span class="st">&quot;VEM&quot;</span>]], <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 
##    2    2    2    3    1    3    3    3    4    3    5    4    1    1    5</code></pre>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="text-mining-natural-language-processing.html#cb408-1" aria-hidden="true" tabindex="-1"></a><span class="fu">terms</span>(TM[[<span class="st">&quot;VEM&quot;</span>]], <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##      Topic 1     Topic 2     Topic 3   Topic 4   Topic 5     
## [1,] &quot;’&quot;         &quot;—&quot;         &quot;clayton&quot; &quot;walter&quot;  &quot;newspap&quot;   
## [2,] &quot;repurchas&quot; &quot;eja&quot;       &quot;deficit&quot; &quot;clayton&quot; &quot;clayton&quot;   
## [3,] &quot;committe&quot;  &quot;repurchas&quot; &quot;see&quot;     &quot;newspap&quot; &quot;repurchas&quot; 
## [4,] &quot;economi&quot;   &quot;merger&quot;    &quot;member&quot;  &quot;iscar&quot;   &quot;journalist&quot;
## [5,] &quot;bnsf&quot;      &quot;ticket&quot;    &quot;—&quot;       &quot;equita&quot;  &quot;monolin&quot;</code></pre>
<p>The output indicates that the first three letter (1998-2000) are about topic 4, the fourth (2001) topic 2, and so on.</p>
<p>Topic 1 is defined by the following terms: thats, bnsf, cant, blacksholes, and railroad. As we have seen previously, some of these words (e.g., thats and cant, which we can infer as being that’s and can’t) are not useful differentiators, and the dictionary could be extended to remove them from consideration and topic modeling repeated. For this particular case, it might be that Buffett’s letters don’t vary much from year to year, and he returns to the same topics in each annual report.</p>
<blockquote>
<p>❓ <em>Skill builder</em></p>
<p>Experiment with the topicmodels package to identify the topics in Buffett’s &gt; letters. You might need to use the dictionary feature of text mining to remove selected words from the corpus to develop a meaningful distinction between topics.</p>
</blockquote>
</div>
</div>
<div id="named-entity-recognition-ner" class="section level2 unnumbered hasAnchor">
<h2>Named-entity recognition (NER)<a href="text-mining-natural-language-processing.html#named-entity-recognition-ner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Named-entity recognition (NER) places terms in a corpus into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, and percentages. It identifies some or all mentions of these categories, as shown in the following figure, where an organization, place, and date are recognized.</p>
<p>Named-entity recognition example</p>
<p><img src="Figures/Chapter%2017/name-entity-recognition.png" /></p>
<p>Tags are added to the corpus to denote the category of the terms identified.</p>
<p>The <organization>Olympics</organization> were in <place>London</place> in <date>2012</date>.</p>
<p>There are two approaches to developing an NER capability. A <em>rules-based</em> approach works well for a well-understood domain, but it requires maintenance and is language dependent. <em>Statistical classifiers</em>, based on machine learning, look at each word in a sentence to decide whether it is the start of a named-entity, a continuation of an already identified named-entity, or not part of a named-entity. Once a named-entity is distinguished, its category (e.g., place) must be identified and surrounding tags inserted.</p>
<p>Statistical classifiers need to be trained on a large collection of human-annotated text that can be used as input to machine learning software. Human-annotation, while time-consuming, does not require a high level of skill. It is the sort of task that is easily parallelized and distributed to a large number of human coders who have a reasonable understanding of the corpus’s context (e.g., able to recognize that London is a place and that the Olympics is an organization). The software classifier need to be trained on approximately 30,000 words.</p>
<p>The accuracy of NER is dependent on the corpus used for training and the domain of the documents to be classified. For example, NER is based on a collection of news stories and is unlikely to be very accurate for recognizing entities in medical or scientific literature. Thus, for some domains, you will likely need to annotate a set of sample documents to create a relevant model. Of course, as times change, it might be necessary to add new annotated text to the learning script to accommodate new organizations, place, people and so forth. A well-trained statistical classifier applied appropriately is usually capable of correctly recognizing entities with 90 percent accuracy.</p>
<div id="ner-software" class="section level3 unnumbered hasAnchor">
<h3>NER software<a href="text-mining-natural-language-processing.html#ner-software" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="http://opennlp.apache.org">OpenNLP</a> is an Apache Java-based machine learning based toolkit for the processing of natural language in text format. It is a collection of natural language processing tools, including a sentence detector, tokenizer, parts-of-speech(POS)-tagger, syntactic parser, and named-entity detector. The NER tool can recognize people, locations, organizations, dates, times. percentages, and money. You will need to write a Java program to take advantage of the toolkit. The R package, openNLP, provides an interface to OpenNLP.</p>
</div>
</div>
<div id="future-developments" class="section level2 unnumbered hasAnchor">
<h2>Future developments<a href="text-mining-natural-language-processing.html#future-developments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Text mining and natural language processing are developing areas and you can expect new tools to emerge. Document summarization, relationship extraction, advanced sentiment analysis, and cross-language information retrieval (e.g., a Chinese speaker querying English documents and getting a Chinese translation of the search and selected documents) are all areas of research that will likely result in generally available software with possible R versions. If you work in this area, you will need to continually scan for new software that extends the power of existing methods and adds new text mining capabilities.</p>
<div id="summary-17" class="section level3 unnumbered hasAnchor">
<h3>Summary<a href="text-mining-natural-language-processing.html#summary-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Language enables cooperation through information exchange. Natural language processing (NLP) focuses on developing and implementing software that enables computers to handle large scale processing of language in a variety of forms, such as written and spoken. The inherent ambiguity in written and spoken speech makes NLP challenging. Don’t expect NLP to provide the same level of exactness and starkness as numeric processing. There are three levels to consider when processing language: semantics, discourse, and pragmatics.</p>
<p>Sentiment analysis is a popular and simple method of measuring aggregate feeling. Tokenization is the process of breaking a document into chunks. A collection of text is called a corpus. The Flesch-Kincaid formula is a common way of assessing readability. Preprocessing, which prepares a corpus for text mining, can include case conversion, punctuation removal, number removal, stripping extra white spaces, stop word filtering, specific word removal, word length filtering, parts of speech (POS) filtering, Stemming, and regex filtering.</p>
<p>Word frequency analysis is a simple technique that can also be the foundation for other analyses. A term-document matrix contains one row for each term and one column for each document. A document-term matrix contains one row for each document and one column for each term. Words that occur frequently within a document are usually a good indicator of the document’s content. A word cloud is way of visualizing the most frequent words. Co-occurrence measures the frequency with which two words appear together. Cluster analysis is a statistical technique for grouping together sets of observations that share common characteristics. Topic modeling is a set of statistical techniques for identifying the topics that occur in a document set. The inverse document frequency (idf) measures the frequency of a term across documents. Named-entity recognition (NER) places terms in a corpus into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, and percentages. Statistical classification is used for NER. OpenNLP is an Apache Java-based machine learning-based toolkit for the processing of natural language in text format. Document summarization, relationship extraction, advanced sentiment analysis, and cross-language information retrieval are all areas of research.</p>
</div>
</div>
<div id="key-terms-and-concepts-13" class="section level2 unnumbered hasAnchor">
<h2>Key terms and concepts<a href="text-mining-natural-language-processing.html#key-terms-and-concepts-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Association</td>
<td>Readability</td>
</tr>
<tr class="even">
<td>Cluster analysis</td>
<td>Regex filtering</td>
</tr>
<tr class="odd">
<td>Co-occurrence</td>
<td>Sentiment analysis</td>
</tr>
<tr class="even">
<td>Corpus</td>
<td>Statistical classification</td>
</tr>
<tr class="odd">
<td>Dendrogram</td>
<td>Stemming</td>
</tr>
<tr class="even">
<td>Document-term matrix</td>
<td>Stop word filtering</td>
</tr>
<tr class="odd">
<td>Flesch-Kincaid formula</td>
<td>Stripping extra white spaces</td>
</tr>
<tr class="even">
<td>Inverse document frequency</td>
<td>Term-document matrix</td>
</tr>
<tr class="odd">
<td>KNIME</td>
<td>Term frequency</td>
</tr>
<tr class="even">
<td>Named-entity recognition (NER)</td>
<td>Term frequency inverse document frequency</td>
</tr>
<tr class="odd">
<td>Natural language processing (NLP)</td>
<td>Text mining</td>
</tr>
<tr class="even">
<td>Number removal</td>
<td>Tokenization</td>
</tr>
<tr class="odd">
<td>OpenNLP</td>
<td>Topic modeling</td>
</tr>
<tr class="even">
<td>Parts of speech (POS) filtering</td>
<td>Word cloud</td>
</tr>
<tr class="odd">
<td>Preprocessing</td>
<td>Word frequency analysis</td>
</tr>
<tr class="even">
<td>Punctuation removal</td>
<td>Word length filtering</td>
</tr>
</tbody>
</table>
<div id="references-2" class="section level3 unnumbered hasAnchor">
<h3>References<a href="text-mining-natural-language-processing.html#references-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Feinerer, I. (2008). An introduction to text mining in R. <em>R News</em>, 8(2), 19-22.</p>
<p>Feinerer, I., Hornik, K., &amp; Meyer, D. (2008). Text mining infrastructure in R. <em>Journal of Statistical Software</em>, 25(5), 1-54.</p>
<p>Grün, B., &amp; Hornik, K. (2011). topicmodels: An R package for fitting topic models. <em>Journal of Statistical Software</em>, 40(13), 1-30.</p>
<p>Ingersoll, G., Morton, T., &amp; Farris, L. (2012). <em>Taming Text: How to find, organize and manipulate it</em>. Greenwich, CT: Manning Publications.</p>
</div>
</div>
<div id="exercises-17" class="section level2 unnumbered hasAnchor">
<h2>Exercises<a href="text-mining-natural-language-processing.html#exercises-17" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Take the recent annual reports for <a href="http://www.investors.ups.com/phoenix.zhtml?c=62900&amp;p=irol-reportsannua">UPS</a> and convert them to text using an online service, <a href="http://www.fileformat.info/convert/doc/pdf2txt.htm">such as</a>.</p>
<p>Complete the following tasks:</p>
<ol style="list-style-type: decimal">
<li><p>Count the words in the most recent annual report.</p></li>
<li><p>Compute the readability of the most recent annual report.</p></li>
<li><p>Create a corpus.</p></li>
<li><p>Preprocess the corpus.</p></li>
<li><p>Create a term-document matrix and compute the frequency of words in the corpus.</p></li>
<li><p>Construct a word cloud for the 25 most common words.</p></li>
<li><p>Undertake a cluster analysis, identify which reports are similar in nature, and see if you can explain why some reports are in different clusters.</p></li>
<li><p>Build a topic model for the annual reports.</p></li>
</ol></li>
<li><p>Merge the annual reports for Berkshire Hathaway (i.e., Buffett’s letters) and UPS into a single corpus.</p>
<ol style="list-style-type: decimal">
<li>Undertake a cluster analysis and identify which reports are similar in nature.</li>
<li>Build a topic model for the combined annual reports.</li>
<li>Do the cluster analysis and topic model suggest considerable differences in the two sets of reports?</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-visualization-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cluster-computing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataManagement.pdf", "DataManagement.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
