<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 18 Cluster computing | DataManagement.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 18 Cluster computing | DataManagement.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 18 Cluster computing | DataManagement.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="text-mining-natural-language-processing.html"/>
<link rel="next" href="dashboards.html"/>
<script src="libs/header-attrs-2.12/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="data-management-databases-and-organizations.html#data-management-databases-and-organizations">Data Management: Databases and Organizations<span></span></a></li>
<li><a href="preface.html#preface">Preface<span></span></a>
<ul>
<li><a href="preface.html#supplements">Supplements<span></span></a></li>
<li><a href="preface.html#acknowledgments">Acknowledgments<span></span></a></li>
</ul></li>
<li><a href="section-1-the-managerial-perspective.html#section-1-the-managerial-perspective">Section 1 The Managerial Perspective<span></span></a></li>
<li class="chapter" data-level="1" data-path="managing-data.html"><a href="managing-data.html"><i class="fa fa-check"></i><b>1</b> Managing Data<span></span></a>
<ul>
<li><a href="managing-data.html#introduction">Introduction<span></span></a></li>
<li><a href="managing-data.html#individual-data-management">Individual data management<span></span></a></li>
<li><a href="managing-data.html#organizational-data-management">Organizational data management<span></span></a></li>
<li><a href="managing-data.html#problems-with-data-management-systems">Problems with data management systems<span></span></a></li>
<li><a href="managing-data.html#a-brief-history-of-data-management-systems">A brief history of data management systems<span></span></a></li>
<li><a href="managing-data.html#data-information-and-knowledge">Data, information, and knowledge<span></span></a></li>
<li><a href="managing-data.html#the-challenge">The challenge<span></span></a></li>
<li><a href="managing-data.html#exercises">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="information.html"><a href="information.html"><i class="fa fa-check"></i><b>2</b> Information<span></span></a>
<ul>
<li><a href="information.html#introduction-1">Introduction<span></span></a></li>
<li><a href="information.html#a-historical-perspective">A historical perspective<span></span></a></li>
<li><a href="information.html#a-brief-history-of-information-systems">A brief history of information systems<span></span></a></li>
<li><a href="information.html#information-characteristics">Information characteristics<span></span></a></li>
<li><a href="information.html#information-and-organizational-change">Information and organizational change<span></span></a></li>
<li><a href="information.html#information-and-managerial-work">Information and managerial work<span></span></a></li>
<li><a href="information.html#managers-information-requirements">Managers’ information requirements<span></span></a></li>
<li><a href="information.html#information-delivery-systems">Information delivery systems<span></span></a></li>
<li><a href="information.html#information-integration">Information integration<span></span></a></li>
<li><a href="information.html#knowledge-1">Knowledge<span></span></a></li>
<li><a href="information.html#exercises-1">Exercises<span></span></a></li>
</ul></li>
<li><a href="section-2-data-modeling-and-sql.html#section-2-data-modeling-and-sql">Section 2 Data Modeling and SQL<span></span></a></li>
<li class="chapter" data-level="3" data-path="the-single-entity.html"><a href="the-single-entity.html"><i class="fa fa-check"></i><b>3</b> The Single Entity<span></span></a>
<ul>
<li><a href="the-single-entity.html#the-relational-model">The relational model<span></span></a></li>
<li><a href="the-single-entity.html#getting-started">Getting started<span></span></a></li>
<li><a href="the-single-entity.html#modeling-a-single-entity-database">Modeling a single-entity database<span></span></a></li>
<li><a href="the-single-entity.html#creating-a-single-table-database">Creating a single-table database<span></span></a></li>
<li><a href="the-single-entity.html#querying-a-single-table-database">Querying a single-table database<span></span></a></li>
<li><a href="the-single-entity.html#exercises-2">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html"><i class="fa fa-check"></i><b>4</b> The One-to-Many Relationship<span></span></a>
<ul>
<li><a href="the-one-to-many-relationship.html#relationships">Relationships<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#creating-a-database-with-a-1m-relationship">Creating a database with a 1:m relationship<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#querying-a-two-table-database">Querying a two-table database<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#regular-expressionpattern-matching-1">Regular expression—pattern matching<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#subqueries-1">Subqueries<span></span></a></li>
<li><a href="the-one-to-many-relationship.html#exercises-3">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html"><i class="fa fa-check"></i><b>5</b> The Many-to-Many Relationship<span></span></a>
<ul>
<li><a href="the-many-to-many-relationship.html#the-many-to-many-relationship-1">The many-to-many relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#creating-a-relational-database-with-an-mm-relationship">Creating a relational database with an m:m relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#querying-an-mm-relationship">Querying an m:m relationship<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#key-terms-and-concepts-2">Key terms and concepts<span></span></a></li>
<li><a href="the-many-to-many-relationship.html#exercises-4">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html"><i class="fa fa-check"></i><b>6</b> One-to-One and Recursive Relationships<span></span></a>
<ul>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-one-to-one-relationship">Modeling a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-one-to-one-relationship">Mapping a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-many-relationship">Mapping a recursive one-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-one-to-one-relationship">Querying a one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-1m-relationship">Querying a recursive 1:m relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-one-to-one-relationship">Modeling a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-one-relationship">Mapping a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-one-to-one-relationship">Querying a recursive one-to-one relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-many-to-many-relationship">Modeling a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-many-to-many-relationship">Mapping a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-many-to-many-relationship">Querying a recursive many-to-many relationship<span></span></a></li>
<li><a href="one-to-one-and-recursive-relationships.html#exercises-5">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-modeling.html"><a href="data-modeling.html"><i class="fa fa-check"></i><b>7</b> Data Modeling<span></span></a>
<ul>
<li><a href="data-modeling.html#modeling">Modeling<span></span></a></li>
<li><a href="data-modeling.html#data-modeling-1">Data modeling<span></span></a></li>
<li><a href="data-modeling.html#data-model-quality">Data model quality<span></span></a></li>
<li><a href="data-modeling.html#quality-improvement">Quality improvement<span></span></a></li>
<li><a href="data-modeling.html#data-modeling-hints">Data modeling hints<span></span></a></li>
<li><a href="data-modeling.html#the-seven-habits-of-highly-effective-data-modelers">The seven habits of highly effective data modelers<span></span></a></li>
<li><a href="data-modeling.html#exercises-6">Exercises<span></span></a></li>
</ul></li>
<li><a href="reference-1-basic-structures.html#reference-1-basic-structures">Reference 1: Basic Structures<span></span></a>
<ul>
<li><a href="reference-1-basic-structures.html#one-entity">One entity<span></span></a></li>
<li><a href="reference-1-basic-structures.html#two-entities">Two entities<span></span></a></li>
<li><a href="reference-1-basic-structures.html#another-entitys-identifier-as-part-of-the-identifier">Another entity’s identifier as part of the identifier<span></span></a></li>
<li><a href="reference-1-basic-structures.html#exercises-7">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html"><i class="fa fa-check"></i><b>8</b> Normalization and Other Data Modeling Methods<span></span></a>
<ul>
<li><a href="normalization-and-other-data-modeling-methods.html#multiple-paths">Multiple paths<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#normalization">Normalization<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#other-data-modeling-methods">Other data modeling methods<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#key-terms-and-concepts-5">Key terms and concepts<span></span></a></li>
<li><a href="normalization-and-other-data-modeling-methods.html#exercises-8">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html"><i class="fa fa-check"></i><b>9</b> The Relational Model and Relational Algebra<span></span></a>
<ul>
<li><a href="the-relational-model-and-relational-algebra.html#background">Background<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#data-structures">Data structures<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#integrity-rules">Integrity rules<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#manipulation-languages">Manipulation languages<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#a-fully-relational-database">A fully relational database<span></span></a></li>
<li><a href="the-relational-model-and-relational-algebra.html#exercises-9">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="sql.html"><a href="sql.html"><i class="fa fa-check"></i><b>10</b> SQL<span></span></a>
<ul>
<li><a href="sql.html#structured-query-language">Structured query language<span></span></a></li>
<li><a href="sql.html#creating-a-table">Creating a table<span></span></a></li>
<li><a href="sql.html#data-types">Data types<span></span></a></li>
<li><a href="sql.html#collation-sequence">Collation sequence<span></span></a></li>
<li><a href="sql.html#scalar-functions">Scalar functions<span></span></a></li>
<li><a href="sql.html#formatting">Formatting<span></span></a></li>
<li><a href="sql.html#table-commands">Table commands<span></span></a></li>
<li><a href="sql.html#data-manipulation">Data manipulation<span></span></a></li>
<li><a href="sql.html#insert">INSERT<span></span></a></li>
<li><a href="sql.html#update-1">UPDATE<span></span></a></li>
<li><a href="sql.html#delete-1">DELETE<span></span></a></li>
<li><a href="sql.html#sql-routines">SQL routines<span></span></a></li>
<li><a href="sql.html#universal-unique-identifier-uuid">Universal Unique Identifier (UUID)<span></span></a></li>
<li><a href="sql.html#nullsmuch-ado-about-missing-information">Nulls—much ado about missing information<span></span></a></li>
<li><a href="sql.html#security">Security<span></span></a></li>
<li><a href="sql.html#the-system-catalog">The system catalog<span></span></a></li>
<li><a href="sql.html#natural-language-processing">Natural language processing<span></span></a></li>
<li><a href="sql.html#connectivity-and-odbc">Connectivity and ODBC<span></span></a></li>
<li><a href="sql.html#embedded-sql">Embedded SQL<span></span></a></li>
<li><a href="sql.html#user-defined-types">User-defined types<span></span></a></li>
<li><a href="sql.html#the-future-of-sql">The future of SQL<span></span></a></li>
</ul></li>
<li><a href="reference-2-sql-playbook.html#reference-2-sql-playbook">Reference 2: SQL Playbook<span></span></a>
<ul>
<li><a href="reference-2-sql-playbook.html#the-power-of-sql">The power of SQL<span></span></a></li>
</ul></li>
<li><a href="section-3-advanced-data-management.html#section-3-advanced-data-management">Section 3 Advanced Data Management<span></span></a></li>
<li class="chapter" data-level="11" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html"><i class="fa fa-check"></i><b>11</b> Spatial and Temporal Data Management<span></span></a>
<ul>
<li><a href="spatial-and-temporal-data-management.html#spatial-data">Spatial data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#managing-spatial-data">Managing spatial data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#data-model-mapping">Data model mapping<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#r-tree">R-tree<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#managing-temporal-data">Managing temporal data<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#key-terms-and-concepts-8">Key terms and concepts<span></span></a></li>
<li><a href="spatial-and-temporal-data-management.html#exercises-11">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="graph-databases.html"><a href="graph-databases.html"><i class="fa fa-check"></i><b>12</b> Graph Databases<span></span></a>
<ul>
<li><a href="graph-databases.html#a-graph-database">A graph database<span></span></a></li>
<li><a href="graph-databases.html#neo4j-a-graph-database-implementation">Neo4j – a graph database implementation<span></span></a></li>
<li><a href="graph-databases.html#a-relationship-between-nodes">A relationship between nodes<span></span></a></li>
<li><a href="graph-databases.html#key-terms-and-concepts-9">Key terms and concepts<span></span></a></li>
<li><a href="graph-databases.html#exercises-12">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="xml-managing-data-exchange.html"><a href="xml-managing-data-exchange.html"><i class="fa fa-check"></i><b>13</b> XML: Managing Data Exchange<span></span></a>
<ul>
<li><a href="xml-managing-data-exchange.html#four-problems">Four problems<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#sgml">SGML<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#xml">XML<span></span></a></li>
<li><a href="xml-managing-data-exchange.html#xml-schema">XML schema<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html"><i class="fa fa-check"></i><b>14</b> Organizational Intelligence<span></span></a>
<ul>
<li><a href="organizational-intelligence.html#information-poverty">Information poverty<span></span></a></li>
<li><a href="organizational-intelligence.html#an-organizational-intelligence-system">An organizational intelligence system<span></span></a></li>
<li><a href="organizational-intelligence.html#the-data-warehouse">The data warehouse<span></span></a></li>
<li><a href="organizational-intelligence.html#exploiting-data-stores">Exploiting data stores<span></span></a></li>
<li><a href="organizational-intelligence.html#data-mining">Data mining<span></span></a></li>
<li><a href="organizational-intelligence.html#exercises-14">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>15</b> Introduction to R<span></span></a>
<ul>
<li><a href="introduction-to-r.html#the-r-project">The R project<span></span></a></li>
<li><a href="introduction-to-r.html#datasets">Datasets<span></span></a></li>
<li><a href="introduction-to-r.html#packages">Packages<span></span></a></li>
<li><a href="introduction-to-r.html#file-handing">File handing<span></span></a></li>
<li><a href="introduction-to-r.html#data-manipulation-with-dplyr">Data manipulation with dplyr<span></span></a></li>
<li><a href="introduction-to-r.html#database-access">Database access<span></span></a></li>
<li><a href="introduction-to-r.html#excel-files">Excel files<span></span></a></li>
<li><a href="introduction-to-r.html#r-resources">R resources<span></span></a></li>
<li><a href="introduction-to-r.html#r-and-data-analytics">R and data analytics<span></span></a></li>
<li><a href="introduction-to-r.html#exercises-15">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="data-visualization-1.html"><a href="data-visualization-1.html"><i class="fa fa-check"></i><b>16</b> Data visualization<span></span></a>
<ul>
<li><a href="data-visualization-1.html#visual-processing">Visual processing<span></span></a></li>
<li><a href="data-visualization-1.html#the-grammar-of-graphics">The grammar of graphics<span></span></a></li>
<li><a href="data-visualization-1.html#ggplot2">ggplot2<span></span></a></li>
<li><a href="data-visualization-1.html#some-recipes">Some recipes<span></span></a></li>
<li><a href="data-visualization-1.html#geographic-data">Geographic data<span></span></a></li>
<li><a href="data-visualization-1.html#r-resources-1">R resources<span></span></a></li>
<li><a href="data-visualization-1.html#exercises-16">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html"><i class="fa fa-check"></i><b>17</b> Text mining &amp; natural language processing<span></span></a>
<ul>
<li><a href="text-mining-natural-language-processing.html#the-nature-of-language">The nature of language<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#levels-of-processing">Levels of processing<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#tokenization">Tokenization<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#sentiment-analysis">Sentiment analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#corpus">Corpus<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#readability">Readability<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#preprocessing">Preprocessing<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#word-frequency-analysis">Word frequency analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#co-occurrence-and-association">Co-occurrence and association<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#cluster-analysis">Cluster analysis<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#topic-modeling">Topic modeling<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#named-entity-recognition-ner">Named-entity recognition (NER)<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#future-developments">Future developments<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#key-terms-and-concepts-13">Key terms and concepts<span></span></a></li>
<li><a href="text-mining-natural-language-processing.html#exercises-17">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="cluster-computing.html"><a href="cluster-computing.html"><i class="fa fa-check"></i><b>18</b> Cluster computing<span></span></a>
<ul>
<li><a href="cluster-computing.html#a-paradigm-shift">A paradigm shift<span></span></a></li>
<li><a href="cluster-computing.html#the-drivers">The drivers<span></span></a></li>
<li><a href="cluster-computing.html#the-bottleneck-and-its-solution">The bottleneck and its solution<span></span></a></li>
<li><a href="cluster-computing.html#lambda-architecture">Lambda Architecture<span></span></a></li>
<li><a href="cluster-computing.html#hadoop">Hadoop<span></span></a></li>
<li><a href="cluster-computing.html#spark">Spark<span></span></a></li>
<li><a href="cluster-computing.html#exercises-18">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="dashboards.html"><a href="dashboards.html"><i class="fa fa-check"></i><b>19</b> Dashboards<span></span></a>
<ul>
<li><a href="dashboards.html#the-value-of-dashboards">The value of dashboards<span></span></a></li>
<li><a href="dashboards.html#designing-a-dashboard">Designing a dashboard<span></span></a></li>
<li><a href="dashboards.html#dashboards-with-r">Dashboards with R<span></span></a></li>
<li><a href="dashboards.html#conclusion-5">Conclusion<span></span></a></li>
<li><a href="dashboards.html#exercises-19">Exercises<span></span></a></li>
</ul></li>
<li><a href="section-4-managing-organizational-memory.html#section-4-managing-organizational-memory">Section 4 Managing Organizational Memory<span></span></a></li>
<li class="chapter" data-level="20" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html"><i class="fa fa-check"></i><b>20</b> Data Structure and Storage<span></span></a>
<ul>
<li><a href="data-structure-and-storage.html#the-data-deluge">The data deluge<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-structures-1">Data structures<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-coding-standards">Data coding standards<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-storage-devices">Data storage devices<span></span></a></li>
<li><a href="data-structure-and-storage.html#data-compression">Data compression<span></span></a></li>
<li><a href="data-structure-and-storage.html#key-terms-and-concepts-15">Key terms and concepts<span></span></a></li>
<li><a href="data-structure-and-storage.html#exercises-20">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html"><i class="fa fa-check"></i><b>21</b> Data Processing Architectures<span></span></a>
<ul>
<li><a href="data-processing-architectures.html#architectural-choices">Architectural choices<span></span></a></li>
<li><a href="data-processing-architectures.html#remote-job-entry">Remote job entry<span></span></a></li>
<li><a href="data-processing-architectures.html#personal-database">Personal database<span></span></a></li>
<li><a href="data-processing-architectures.html#clientserver">Client/server<span></span></a></li>
<li><a href="data-processing-architectures.html#cloud-computing">Cloud computing<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-database">Distributed database<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-data-access">Distributed data access<span></span></a></li>
<li><a href="data-processing-architectures.html#distributed-database-design">Distributed database design<span></span></a></li>
<li><a href="data-processing-architectures.html#key-terms-and-concepts-16">Key terms and concepts<span></span></a></li>
<li><a href="data-processing-architectures.html#exercises-21">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="sql-and-java.html"><a href="sql-and-java.html"><i class="fa fa-check"></i><b>22</b> SQL and Java<span></span></a>
<ul>
<li><a href="sql-and-java.html#java">JAVA<span></span></a></li>
<li><a href="sql-and-java.html#using-sql-within-java">Using SQL within Java<span></span></a></li>
<li><a href="sql-and-java.html#javaserver-pages-jsp">JavaServer Pages (JSP)<span></span></a></li>
<li><a href="sql-and-java.html#exercises-22">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-integrity.html"><a href="data-integrity.html"><i class="fa fa-check"></i><b>23</b> Data Integrity<span></span></a>
<ul>
<li><a href="data-integrity.html#introduction-2">Introduction<span></span></a></li>
<li><a href="data-integrity.html#transaction-management">Transaction management<span></span></a></li>
<li><a href="data-integrity.html#protecting-existence">Protecting existence<span></span></a></li>
<li><a href="data-integrity.html#maintaining-data-quality">Maintaining data quality<span></span></a></li>
<li><a href="data-integrity.html#ensuring-confidentiality">Ensuring confidentiality<span></span></a></li>
<li><a href="data-integrity.html#key-terms-and-concepts-18">Key terms and concepts<span></span></a></li>
<li><a href="data-integrity.html#exercises-23">Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="data-administration.html"><a href="data-administration.html"><i class="fa fa-check"></i><b>24</b> Data Administration<span></span></a>
<ul>
<li><a href="data-administration.html#introduction-3">Introduction<span></span></a></li>
<li><a href="data-administration.html#the-chief-data-officer">The Chief Data Officer<span></span></a></li>
<li><a href="data-administration.html#management-of-the-database-environment">Management of the database environment<span></span></a></li>
<li><a href="data-administration.html#data-administration-1">Data administration<span></span></a></li>
<li><a href="data-administration.html#database-management-systems-dbmss">Database management systems (DBMSs)<span></span></a></li>
<li><a href="data-administration.html#groupware-1">Groupware<span></span></a></li>
<li><a href="data-administration.html#data-integration">Data integration<span></span></a></li>
<li><a href="data-administration.html#conclusion-9">Conclusion<span></span></a></li>
<li><a href="data-administration.html#exercises-24">Exercises<span></span></a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cluster-computing" class="section level1 hasAnchor" number="18">
<h1><span class="header-section-number">Chapter 18</span> Cluster computing<a href="cluster-computing.html#cluster-computing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p><em>Let us change our traditional attitude to the construction of
programs: Instead of imagining that our main task is to instruct a
computer what to do, let us concentrate rather on explaining to humans
what we want the computer to do</em></p>
<p>Donald E. Knuth, <em>Literate Programming</em>, 1984</p>
</blockquote>
<div id="learning-objectives-17" class="section level3 unnumbered hasAnchor">
<h3>Learning objectives<a href="cluster-computing.html#learning-objectives-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Students completing this chapter will:</p>
<ul>
<li><p>Understand the paradigm shift in decision-oriented data processing;</p></li>
<li><p>Understand the principles of cluster computing</p></li>
</ul>
</div>
<div id="a-paradigm-shift" class="section level2 unnumbered hasAnchor">
<h2>A paradigm shift<a href="cluster-computing.html#a-paradigm-shift" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is much talk of big data, and much of it is not very informative.
Rather a lot of big talk but not much smart talk. Big data is not just
about greater variety and volumes of data at higher velocity, which is
certainly occurring. The more important issue is the paradigm shift in
data processing so that large volumes of data can be handled in a timely
manner to support decision making. The foundations of this new model is
the shift to cluster computing, which means using large numbers of
commodity processors for massively parallel computing.</p>
<p>We start by considering what is different between the old and new
paradigms for decision data analysis. Note that we are not considering
transaction processing, for which the relational model is a sound
solution. Rather, we are interested in the processing of very large
volumes of data at a time, and the relational model was not designed for
this purpose. It is suited for handling transactions, which typically
involve only a few records. The multidimensional database (MDDB) is the
“old” approach for large datasets and cluster compute is the “new.”</p>
<p>Another difference is the way data are handled. The old approach is to
store data on a high speed disk drive and load it into computer memory
for processing. To speed up processing, the data might be moved to
multiple computers to enable parallel processing and the results merged
into a single file. Because data files are typically much larger than
programs, moving data from disks to computers is time consuming. Also,
high performance disk storage devices are expensive. The new method is
to spread a large data file across multiple commodity computers,
possibly using <em>Hadoop Distributed File System</em> (HDFS), and then send each computer a copy of the program
to run in parallel. The results from the individual jobs are then
merged. While data still need to be moved to be processed, they are
moved across a high speed data channel within a computer rather than the
lower speed cables of a storage network.</p>
<table>
<thead>
<tr class="header">
<th align="left">Old</th>
<th align="left">New</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Data to the program</td>
<td align="left">Program to the data</td>
</tr>
<tr class="even">
<td align="left">Mutable data</td>
<td align="left">Immutable data</td>
</tr>
<tr class="odd">
<td align="left">Special purpose hardware</td>
<td align="left">Commodity hardware</td>
</tr>
</tbody>
</table>
</div>
<div id="the-drivers" class="section level2 unnumbered hasAnchor">
<h2>The drivers<a href="cluster-computing.html#the-drivers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Exploring the drivers promoting the paradigm shift is a good starting
point for understanding this important change in data management. First,
you will recall that you learned in Chapter 1 that decision making is
the central organizational activity. Furthermore, because data-driven
decision making increases organizational performance, many executives
are now demanding data analytics to support their decision making.</p>
<p>Second, as we also explained in Chapter 1, there is a societal shift in
dominant logic as we collectively recognize that we need to focus on
reducing environmental degradation and carbon emissions. Service and
sustainability dominant logics are both data intensive. Customer service
decisions are increasingly based on the analysis of large volumes of
operational and social data. Sustainability oriented decisions also
require large volumes of operational data, which are combined with
environmental data collected by massive sensor networks to support
decision making that reduces an organization’s environmental impact.</p>
<p>Third, the world is in the midst of a massive data generating digital
transformation. Large volumes of data are collected about the operation
on an aircraft’s jet engines, how gamers play massively online games,
how people interact in social media space, and the operation of cell
phone networks, for example. The digital transformation of life and work
is creating a bits and bytes tsunami.</p>
</div>
<div id="the-bottleneck-and-its-solution" class="section level2 unnumbered hasAnchor">
<h2>The bottleneck and its solution<a href="cluster-computing.html#the-bottleneck-and-its-solution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a highly connected and competitive world, speedy high quality
decisions can be a competitive advantage. However, large data sets can
take some time and expense to process, and so as more data are
collected, there is a the danger that decision making will gradually
slow down and its quality lowered. Data analytics becomes a bottleneck
when the conversion of data to information is too slow. Second, decision
quality is lowered when there is a dearth of skills for determining what
data should be converted to information and interpreting the resulting
conversion. We capture these problems in the elaboration of a diagram
that was introduced in Chapter 1, which now illustrates the causes of
the conversion, request, and interpretation bottlenecks.</p>
<p><em>Data analytics bottleneck</em></p>
<p><img src="Figures/Chapter%2018/bottleneck.png" /></p>
<p>The people skills problem is being addressed by the many universities
that have added graduate courses in data analytics. The <strong>Lambda
Architecture</strong><a href="data-administration.html#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> is a proposed general solution to
the speed and cost problem.</p>
</div>
<div id="lambda-architecture" class="section level2 unnumbered hasAnchor">
<h2>Lambda Architecture<a href="cluster-computing.html#lambda-architecture" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now consider the three layers of the Lambda Architecture: batch,
speed, and serving.</p>
<div id="the-batch-layer" class="section level3 unnumbered hasAnchor">
<h3>The batch layer<a href="cluster-computing.html#the-batch-layer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Batch computing describes the situation where a computer works on one or
more large tasks with minimal interruption. Because early computers were
highly expensive and businesses operated at a different tempo, batch
computing was common in the early days of information systems. The
efficiency gains of batch computing mainly come from uninterrupted
sequential file processing. The computer spends less time waiting for
data to be retrieved from disks, particularly with Hadoop where files are
stored in 64Mb chunks. Batch computing is very efficient, though not timely,
and the Lambda Architecture takes advantage of this efficiency.</p>
<p>The batch layer is used to precompute queries by running them with the
most recent version of a dataset. The precomputed results are saved
and can then be used as required. For example, a supermarket chain might
want to know how often each pair of products appears in each shopper’s
basket for each day for each store. These data might help it to set
cross-promotional activities within a store (e.g., a joint special on
steak and mashed potatoes). The batch program could precompute the count
of joint sales for each pair of items for each day for each store in a
given date range. This highly summarized data could then be used for
queries about customers’ baskets (e.g., how many customers purchased
both shrimp and grits in the last week in all Georgia stores?). The
batch layer works with a dataset essentially consisting of every
supermarket receipt because this is the record of a customer’s basket.
This dataset is also stored by the batch layer. HDFS is well-suited
for handling the batch layer, as you will see later, but it is not the
only option.</p>
<p>New data are appended to the master dataset to preserve is immutability,
so that it remains a complete record of transactions for a particular
domain (e.g., all receipts). These incremental data are processed the
next time the batch process is restarted.</p>
<p>The batch layer can be processing several batches simultaneously. It
typically keeps recomputing batch views using the latest dataset every
few hours or maybe overnight.</p>
</div>
<div id="the-serving-layer" class="section level3 unnumbered hasAnchor">
<h3>The serving layer<a href="cluster-computing.html#the-serving-layer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The serving layer processes views computed by the batch layer so they
can be queried. Because the batch layer produces a flat file, the
serving layer indexes it for random access. The serving layer also
replaces the old batch output with the latest indexed batch view when it
is received from the batch layer. In a typical Lambda Architecture
system, there might be several or more hours between batch updates.</p>
<p>The combination of the batch and serving layers provides for efficient
reporting, but it means that any queries on the files generated by the
batch layer might be several or more hours old. We have efficiency but
not timeliness, for which we need the speed layer.</p>
</div>
<div id="speed-layer" class="section level3 unnumbered hasAnchor">
<h3>Speed layer<a href="cluster-computing.html#speed-layer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once a batch recompute has started running, all newly collected data
cannot be part of the resulting batch report. The purpose of the speed
layer is to process incremental data as they arrive so they can be
merged with the latest batch data report to give current results.
Because the speed layer modifies the results as each chunk of data
(e.g., a transaction) is received, the merge of the batch and speed
layer computations can be used to create real-time reports.</p>
<p><em>Merging speed and serving layer results to create a report (source:
(Marz and Warren 2012))</em></p>
<p><img src="Figures/Chapter%2018/merge-speed-serving.png" /></p>
</div>
<div id="putting-the-layers-together" class="section level3 unnumbered hasAnchor">
<h3>Putting the layers together<a href="cluster-computing.html#putting-the-layers-together" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now examine the process in detail.</p>
<p>Assume batch<sub>n-1</sub> has just been processed.</p>
<ol style="list-style-type: decimal">
<li><p>During the processing of batch<sub>n-1</sub>, increment<sub>n-1</sub> was created from
the records received. The combination of these two data sets creates
batch<sub>n</sub>.</p></li>
<li><p>As the data for increment<sub>n-1</sub> were received, speed layer
(Sresults<sub>n-1</sub>) were dynamically recomputed.</p></li>
<li><p>A current report can be created by combining speed layer and batch
layer results (i.e., Sresults<sub>n-1</sub> and Bresults<sub>n-1</sub>).</p></li>
</ol>
<p>Now, assume batch computation resumes with batch<sub>n</sub>.</p>
<ol style="list-style-type: decimal">
<li><p>Sresults<sub>n</sub> are computed from the data collected (increment<sub>n</sub>)
while batch<sub>n</sub> is being processing.</p></li>
<li><p>Current reports are based on Bresults<sub>n-1</sub>, Sresults<sub>n-1</sub>, and
Sresults<sub>n</sub>.</p></li>
<li><p>At the end of processing batch<sub>n</sub>, Sresults<sub>n-1</sub> can be discarded
because Bresults<sub>n</sub> includes all the data from batch<sub>n-1</sub> and
increment<sub>n-1.</sub></p></li>
</ol>
<p><em>The preparation of a real-time report using batch and speed layer
results when processing batch<sub>n</sub></em></p>
<p><img src="Figures/Chapter%2018/lambda-processing.png" /></p>
<p>In summary, the batch layer pre-computes reports using all the currently
available data. The serving layer indexes the results of the batch
layers and creates views that are the foundation for rapid responses to
queries. The speed layer does incremental updates as data are received.
Queries are handled by merging data from the serving and speed layers.</p>
<p><em>Lambda Architecture (source: (Marz and Warren 2012))</em></p>
<p><img src="Figures/Chapter%2018/lambda.png" /></p>
</div>
<div id="benefits-of-the-lambda-architecture" class="section level3 unnumbered hasAnchor">
<h3>Benefits of the Lambda Architecture<a href="cluster-computing.html#benefits-of-the-lambda-architecture" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Lambda Architecture provides some important advantages for
processing large datasets, and these are now considered.</p>
<div id="robust-and-fault-tolerant" class="section level4 unnumbered hasAnchor">
<h4>Robust and fault-tolerant<a href="cluster-computing.html#robust-and-fault-tolerant" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Programming for batch processing is relatively simple and also it can
easily be restarted if there is a problem. Replication of the batch
layer dataset across computers increases fault tolerance. If a block is
unreadable, the batch processor can shift to the identical block on
another node in the cluster. Also, the redundancy intrinsic to a
distributed file system and distributed processors provides
fault-tolerance.</p>
<p>The speed layer is the complex component of the Lambda Architecture.
Because complexity is isolated to this layer, it does not impact other
layers. In addition, since the speed layer produced temporary results,
these can be discarded in the event of an error. Eventually the system
will right itself when the batch layer produces a new set of results,
though intermediate reports might be a little out of date.</p>
</div>
<div id="low-latency-reads-and-updates" class="section level4 unnumbered hasAnchor">
<h4>Low latency reads and updates<a href="cluster-computing.html#low-latency-reads-and-updates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The speed layer overcomes the long delays associated with batch
processing. Real-time reporting is possible through the combination of
batch and speed layer outputs.</p>
</div>
<div id="scalable" class="section level4 unnumbered hasAnchor">
<h4>Scalable<a href="cluster-computing.html#scalable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Scalability is achieved using a distributed file system and distributed
processors. To scale, new computers with associated disk storage are added.</p>
</div>
<div id="support-a-wide-variety-of-applications" class="section level4 unnumbered hasAnchor">
<h4>Support a wide variety of applications<a href="cluster-computing.html#support-a-wide-variety-of-applications" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The general architecture can support reporting for a wide variety of
situations.</p>
</div>
<div id="extensible" class="section level4 unnumbered hasAnchor">
<h4>Extensible<a href="cluster-computing.html#extensible" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>New data types can be added to the master dataset or new master datasets
created. Furthermore, new computations can be added to the batch and
speed layers to create new views.</p>
</div>
<div id="ad-hoc-queries" class="section level4 unnumbered hasAnchor">
<h4>Ad hoc queries<a href="cluster-computing.html#ad-hoc-queries" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>On the fly queries can be run on the output of the batch layer provided
the required data are available in a view.</p>
</div>
<div id="minimal-maintenance" class="section level4 unnumbered hasAnchor">
<h4>Minimal maintenance<a href="cluster-computing.html#minimal-maintenance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The batch and serving layers are relatively simple programs because they
don’t deal with random updates or writes. Simpler code requires less
maintenance.</p>
</div>
<div id="debuggable" class="section level4 unnumbered hasAnchor">
<h4>Debuggable<a href="cluster-computing.html#debuggable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Batch programs are easier to debug because you can have a clear link
between the input and output. Data immutability means that debugging is
easier because no records have been overwritten during batch processing.</p>
</div>
</div>
<div id="relational-and-lambda-architectures" class="section level3 unnumbered hasAnchor">
<h3>Relational and Lambda Architectures<a href="cluster-computing.html#relational-and-lambda-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Relational technology supports both transaction processing and data
analytics. As a result, it needs to be more complex than the Lambda
Architecture. Separating out data analytics from transaction processing
simplifies the supporting technology and makes it suitable for handling
large volumes of data efficiently. Relational systems can continue to
support transaction processing and, as a byproduct, produce data that
are fed to Lambda Architecture based business analytics.</p>
</div>
</div>
<div id="hadoop" class="section level2 unnumbered hasAnchor">
<h2>Hadoop<a href="cluster-computing.html#hadoop" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="http://hadoop.apache.org">Hadoop</a>, an Apache project, supports distributed
processing of large data sets across a cluster of computers. A Hadoop
cluster consists of many standard processors, nodes, with associated
main memory and disks. They are connected by Ethernet or switches so
they can pass data from node to node. Hadoop is highly scalable and
reliable. It is a suitable technology for the batch layer of the Lambda
architecture. Hadoop is a foundation for data analytics, machine
learning, search ranking, email anti-spam, ad optimization, and other
areas of applications which are constantly emerging.</p>
<p>A market analysis projects that the Hadoop market is growing at 48% per
year.<a href="data-administration.html#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a> An early study<a href="data-administration.html#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a> asserts that, “Hadoop is the only
cost-sensible and scalable open source alternative to commercially
available Big Data management packages. It also becomes an integral part
of almost any commercially available Big Data solution and de-facto
industry standard for business intelligence (BI).”</p>
<div id="hadoop-distributed-file-system-hdfs" class="section level3 unnumbered hasAnchor">
<h3>Hadoop distributed file system (HDFS)<a href="cluster-computing.html#hadoop-distributed-file-system-hdfs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>HDFS is a highly scalable, fault-toleration, distributed file system.
When a file is uploaded to HDFS, it is split into fixed sized blocks of
at least 64MB. Blocks are replicated across nodes to support parallel
processing and provide fault tolerance. As the following diagram
illustrates, an original file when written to HDFS is broken into
multiple large blocks that are spread across multiple nodes. HDFS
provides a set of functions for converting a file to and from HDFS
format and handling HDFS.</p>
<p><em>Splitting of file across a HDFS cluster.</em></p>
<p><img src="Figures/Chapter%2018/splitting-cluster.png" /></p>
<p>On each node, blocks are stored sequentially to minimize disk head
movement. Blocks are grouped into files, and all files for a dataset are
grouped into a single folder. As part of the simplification to support
batch processing, there is no random access to records and new data are
added as a new file.</p>
<p>Scalability is facilitated by the addition of new nodes, which means
adding a few more pieces of inexpensive commodity hardware to the
cluster. Appending new data as files on the cluster also supports
scalability.</p>
<p>HDFS also supports partitioning of data into folders for processing at
the folder level. For example, you might want all employment related
data in a single folder.</p>
</div>
</div>
<div id="spark" class="section level2 unnumbered hasAnchor">
<h2>Spark<a href="cluster-computing.html#spark" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="https://spark.apache.org">Spark</a> is an Apache project for cluster computing
that was initiated at the University of California Berkeley and later
transferred to Apache as an open source project. Spark’s distributed
file system, <em>resilient distributed dataset</em> (RDD), has similar
characteristics to HDFS. Spark can also interface with HDFS and other
distributed file systems. For testing and development, Spark has a local
mode, that can work with a local file system.</p>
<p>Spark includes several component, including Spark SQL for SQL-type
queries, Spark streaming for real-time analysis of event data as it is
received, and a machine learning (ML) library. This library is designed
for in-memory processing and is approximately 10 times faster than
disk-based equivalent approaches. Distributed graph processing is
implemented using GraphX.</p>
<div id="computation-with-spark" class="section level3 unnumbered hasAnchor">
<h3>Computation with Spark<a href="cluster-computing.html#computation-with-spark" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Spark applications can be written in Java, Scala, Python, and R. In our
case, we will use <a href="http://spark.rstudio.com">sparklyr</a>, an R interface to
Spark. This package provides a simple, easy to use set of commands for
exposing the distributed processing power of Spark to those familiar
with R. In particular, it supports dplyr for data manipulation of Spark
datasets and access to Sparks ML library.</p>
<p>Before starting with sparklyr, you need to check that you have latest
version of <a href="https://www.java.com/en/download/help/version_manual.xml">Java</a> on your machine. Use RStudio to install sparklyr. For developing and testing on your computer, install a
local version of Spark.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="cluster-computing.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sparklyr)</span>
<span id="cb410-2"><a href="cluster-computing.html#cb410-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spark_install</span>(<span class="at">version=</span><span class="st">&#39;2.4&#39;</span>)</span></code></pre></div>
<p>Use the <code>spark_connect</code> function to connect to Spark either locally or
on a remote Spark cluster. The following code shows how to specify local
Spark connection (sc).</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="cluster-computing.html#cb411-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sparklyr)</span>
<span id="cb411-2"><a href="cluster-computing.html#cb411-2" aria-hidden="true" tabindex="-1"></a>sc <span class="ot">&lt;-</span> <span class="fu">spark_connect</span>(<span class="at">master =</span> <span class="st">&quot;local&quot;</span>)</span></code></pre></div>
</div>
<div id="tabulation" class="section level3 unnumbered hasAnchor">
<h3>Tabulation<a href="cluster-computing.html#tabulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this example, we have a list of average monthly temperatures for <a href="http://www.erh.noaa.gov/okx/climate/records/monthannualtemp.html">New
York’s Central Park</a> and we want to determine how
often each particular temperature occurred.</p>
<p>Average monthly temperatures since 1869 are read, and temperature is
rounded to an integer for the convenience of tabulation.</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="cluster-computing.html#cb412-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb412-2"><a href="cluster-computing.html#cb412-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb412-3"><a href="cluster-computing.html#cb412-3" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span>  <span class="st">&quot;http://www.richardtwatson.com/data/centralparktemps.txt&quot;</span></span>
<span id="cb412-4"><a href="cluster-computing.html#cb412-4" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(url, <span class="at">delim=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb412-5"><a href="cluster-computing.html#cb412-5" aria-hidden="true" tabindex="-1"></a><span class="co"># tabulate frequencies for temperature</span></span>
<span id="cb412-6"><a href="cluster-computing.html#cb412-6" aria-hidden="true" tabindex="-1"></a>t <span class="sc">%&gt;%</span></span>
<span id="cb412-7"><a href="cluster-computing.html#cb412-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Fahrenheit =</span> <span class="fu">round</span>(temperature,<span class="dv">0</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb412-8"><a href="cluster-computing.html#cb412-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Fahrenheit) <span class="sc">%&gt;%</span></span>
<span id="cb412-9"><a href="cluster-computing.html#cb412-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">Frequency =</span> <span class="fu">n</span>())</span></code></pre></div>
</div>
<div id="spark-example" class="section level3 unnumbered hasAnchor">
<h3>Spark example<a href="cluster-computing.html#spark-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By using dplyr in the prior R code, we can copy and paste and add a few
commands for the Spark implementation. The major differences are the
creation of a Spark connection (sc) and copying the R tibble to Spark
with copy-to. Also, note that you need to sort the resulting tibble,
which is not required in regular R.</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="cluster-computing.html#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb413-2"><a href="cluster-computing.html#cb413-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb413-3"><a href="cluster-computing.html#cb413-3" aria-hidden="true" tabindex="-1"></a><span class="fu">spark_install</span>(<span class="at">version=</span><span class="st">&#39;2.4&#39;</span>)</span>
<span id="cb413-4"><a href="cluster-computing.html#cb413-4" aria-hidden="true" tabindex="-1"></a>sc <span class="ot">&lt;-</span> <span class="fu">spark_connect</span>(<span class="at">master =</span> <span class="st">&quot;local&quot;</span>, <span class="at">spark_home=</span><span class="fu">spark_home_dir</span>(<span class="at">version =</span> <span class="st">&#39;2.4&#39;</span>))</span>
<span id="cb413-5"><a href="cluster-computing.html#cb413-5" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span>  <span class="st">&quot;http://www.richardtwatson.com/data/centralparktemps.txt&quot;</span></span>
<span id="cb413-6"><a href="cluster-computing.html#cb413-6" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(url, <span class="at">delim=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb413-7"><a href="cluster-computing.html#cb413-7" aria-hidden="true" tabindex="-1"></a>t_tbl <span class="ot">&lt;-</span> <span class="fu">copy_to</span>(sc,t)</span>
<span id="cb413-8"><a href="cluster-computing.html#cb413-8" aria-hidden="true" tabindex="-1"></a>t_tbl <span class="sc">%&gt;%</span></span>
<span id="cb413-9"><a href="cluster-computing.html#cb413-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Fahrenheit =</span> <span class="fu">round</span>(temperature,<span class="dv">0</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb413-10"><a href="cluster-computing.html#cb413-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Fahrenheit) <span class="sc">%&gt;%</span></span>
<span id="cb413-11"><a href="cluster-computing.html#cb413-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">Frequency =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb413-12"><a href="cluster-computing.html#cb413-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(Fahrenheit)</span></code></pre></div>
<p>It you observe the two sets of output carefully, you will note that the
results are not identical. It is because rounding can vary across
systems. The IEEE Standard for Floating-Point Arithmetic <a href="https://en.wikipedia.org/wiki/IEEE_floating_point">IEEE
754</a> states on rounding, “if the number falls
midway it is rounded to the nearest value with an even (zero) least
significant bit.” Compare the results for round(12.5,0) and
round(13.5,0). R follows the IEEE standard, but Spark apparently does
not.</p>
<blockquote>
<p>❓ <em>Skill builder</em></p>
<p>Aedo the tabulation example with temperatures in
Celsius.</p>
</blockquote>
</div>
<div id="basic-statistics-with-spark" class="section level3 unnumbered hasAnchor">
<h3>Basic statistics with Spark<a href="cluster-computing.html#basic-statistics-with-spark" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now take the same temperature dataset and calculate mean, min, and
max monthly average temperatures for each year and put the results in a
single file.</p>
<div id="spark-example-1" class="section level4 unnumbered hasAnchor">
<h4>Spark example<a href="cluster-computing.html#spark-example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="cluster-computing.html#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb414-2"><a href="cluster-computing.html#cb414-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb414-3"><a href="cluster-computing.html#cb414-3" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span>  <span class="st">&quot;http://www.richardtwatson.com/data/centralparktemps.txt&quot;</span></span>
<span id="cb414-4"><a href="cluster-computing.html#cb414-4" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(url, <span class="at">delim=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb414-5"><a href="cluster-computing.html#cb414-5" aria-hidden="true" tabindex="-1"></a><span class="co"># report minimum, mean, and maximum by year</span></span>
<span id="cb414-6"><a href="cluster-computing.html#cb414-6" aria-hidden="true" tabindex="-1"></a>t <span class="sc">%&gt;%</span></span>
<span id="cb414-7"><a href="cluster-computing.html#cb414-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(year) <span class="sc">%&gt;%</span></span>
<span id="cb414-8"><a href="cluster-computing.html#cb414-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">Min=</span><span class="fu">min</span>(temperature),</span>
<span id="cb414-9"><a href="cluster-computing.html#cb414-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">Mean =</span> <span class="fu">round</span>(<span class="fu">mean</span>(temperature),<span class="dv">1</span>),</span>
<span id="cb414-10"><a href="cluster-computing.html#cb414-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">Max =</span> <span class="fu">max</span>(temperature))</span></code></pre></div>
</div>
<div id="spark-example-2" class="section level4 unnumbered hasAnchor">
<h4>Spark example<a href="cluster-computing.html#spark-example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Again, the use of dplyr makes the conversion to Spark simple.</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="cluster-computing.html#cb415-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sparklyr)</span>
<span id="cb415-2"><a href="cluster-computing.html#cb415-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb415-3"><a href="cluster-computing.html#cb415-3" aria-hidden="true" tabindex="-1"></a><span class="fu">spark_install</span>(<span class="at">version=</span><span class="st">&#39;2.4&#39;</span>)</span>
<span id="cb415-4"><a href="cluster-computing.html#cb415-4" aria-hidden="true" tabindex="-1"></a>sc <span class="ot">&lt;-</span> <span class="fu">spark_connect</span>(<span class="at">master =</span> <span class="st">&quot;local&quot;</span>, <span class="at">spark_home=</span><span class="fu">spark_home_dir</span>(<span class="at">version =</span> <span class="st">&#39;2.4&#39;</span>))</span>
<span id="cb415-5"><a href="cluster-computing.html#cb415-5" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span>  <span class="st">&quot;http://www.richardtwatson.com/data/centralparktemps.txt&quot;</span></span>
<span id="cb415-6"><a href="cluster-computing.html#cb415-6" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(url, <span class="at">delim=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb415-7"><a href="cluster-computing.html#cb415-7" aria-hidden="true" tabindex="-1"></a>t_tbl <span class="ot">&lt;-</span> <span class="fu">copy_to</span>(sc,t)</span>
<span id="cb415-8"><a href="cluster-computing.html#cb415-8" aria-hidden="true" tabindex="-1"></a><span class="co"># report minimum, mean, and maximum by year</span></span>
<span id="cb415-9"><a href="cluster-computing.html#cb415-9" aria-hidden="true" tabindex="-1"></a><span class="co"># note that sparkly gives a warning if you do not specify how to handle missing values</span></span>
<span id="cb415-10"><a href="cluster-computing.html#cb415-10" aria-hidden="true" tabindex="-1"></a>t_tbl <span class="sc">%&gt;%</span></span>
<span id="cb415-11"><a href="cluster-computing.html#cb415-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(year) <span class="sc">%&gt;%</span></span>
<span id="cb415-12"><a href="cluster-computing.html#cb415-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">Min=</span><span class="fu">min</span>(temperature, <span class="at">na.rm =</span> T),</span>
<span id="cb415-13"><a href="cluster-computing.html#cb415-13" aria-hidden="true" tabindex="-1"></a>            <span class="at">Mean =</span> <span class="fu">round</span>(<span class="fu">mean</span>(temperature, <span class="at">na.rm =</span> T),<span class="dv">1</span>),</span>
<span id="cb415-14"><a href="cluster-computing.html#cb415-14" aria-hidden="true" tabindex="-1"></a>            <span class="at">Max =</span> <span class="fu">max</span>(temperature, <span class="at">na.rm =</span> T)) <span class="sc">%&gt;%</span></span>
<span id="cb415-15"><a href="cluster-computing.html#cb415-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(year)</span></code></pre></div>
<blockquote>
<p>❓ <strong>Skill builder</strong></p>
<p>A [file][<a href="http://people.terry.uga.edu/rwatson/data/electricityprices2010_14.csv" class="uri">http://people.terry.uga.edu/rwatson/data/electricityprices2010_14.csv</a> of electricity costs for
a major city contains a timestamp and cost separated by a comma.
Compute the minimum, mean, and maximum costs.</p>
</blockquote>
</div>
</div>
<div id="summary-18" class="section level3 unnumbered hasAnchor">
<h3>Summary<a href="cluster-computing.html#summary-18" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Big data is a paradigm shift to new file structures, such as HDFS and
RDD, and algorithms for the parallel processing of large volumes of
data. The new file structure approach is to spread a large data file
across multiple commodity computers and then send each computer a copy
of the program to run in parallel. The drivers of the transformation are
the need for high quality data-driven decisions, a societal shift in
dominant logic, and a digital transformation. The speed and cost of
converting data to information is a critical bottleneck as is a dearth
of skills for determining what data should be converted to information
and interpreting the resulting conversion. The people skills problem is
being addressed by universities’ graduate courses in data analytics. The
Lambda Architecture, a solution for handling the speed and cost problem,
consists of three layers: speed, serving, and batch. The batch layer is
used to precompute queries by running them with the most recent version
of the dataset. The serving layer processes views computed by the batch
layer so they can be queried. The purpose of the speed layer is to
process incremental data as they arrive so they can be merged with the
latest batch data report to give current results. The Lambda
Architecture provides some important advantages for processing large
datasets. Relational systems can continue to support transaction
processing and, as a byproduct, produce data that are fed to Lambda
Architecture based business analytics.</p>
<p>Hadoop supports distributed processing of large data sets across a
cluster of computers. A Hadoop cluster consists of many standard
processors, nodes, with associated main memory and disks. HDFS is a
highly scalable, fault-toleration, distributed file system. Spark is a
distributed computing method for scalable and fault-tolerant cluster
computation.</p>
</div>
<div id="key-terms-and-concepts-14" class="section level3 unnumbered hasAnchor">
<h3>Key terms and concepts<a href="cluster-computing.html#key-terms-and-concepts-14" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<tbody>
<tr class="odd">
<td align="left">Batch layer</td>
<td align="left">Lambda Architecture</td>
</tr>
<tr class="even">
<td align="left">Bottleneck</td>
<td align="left">Parallelism</td>
</tr>
<tr class="odd">
<td align="left">Cluster computing</td>
<td align="left">Serving layer</td>
</tr>
<tr class="even">
<td align="left">Hadoop</td>
<td align="left">Spark</td>
</tr>
<tr class="odd">
<td align="left">HDFS</td>
<td align="left">Speed layer</td>
</tr>
<tr class="even">
<td align="left">Immutable data</td>
<td align="left"></td>
</tr>
</tbody>
</table>
</div>
<div id="references-and-additional-readings-9" class="section level3 unnumbered hasAnchor">
<h3>References and additional readings<a href="cluster-computing.html#references-and-additional-readings-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Lam, C. (2010). <em>Hadoop in action</em>: Manning Publications.</p>
<p>Marz, N., &amp; Warren, J. (2012). <em>Big Data</em>: Manning Publications.</p>
</div>
</div>
<div id="exercises-18" class="section level2 unnumbered hasAnchor">
<h2>Exercises<a href="cluster-computing.html#exercises-18" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Write Spark code for the following situations.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Compute the square and cube of the numbers in the range 1 to 25.
Display the results in a data frame.</p></li>
<li><p>Using the average monthly temperatures for New York’s Central
Park, compute the maximum, mean, and average temperature in
Celsius for each month.</p></li>
<li><p>Using the average monthly temperatures for New York’s Central
Park, compute the max, mean, and min for August. You will need
to use subsetting, as discussed in this chapter.</p></li>
<li><p>Using the electricity price data, compute the average hourly
cost.</p></li>
<li><p>Read the national <a href="http://www.richardtwatson.com/data/GDP.csv">GDP file</a>, which records
GDP in millions, and count how many countries have a GDP greater
than or less than 10,000 million.</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="text-mining-natural-language-processing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dashboards.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataManagement.pdf", "DataManagement.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
