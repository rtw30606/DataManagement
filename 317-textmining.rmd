---
editor_options: 
  markdown: 
    wrap: 72
---

# Text mining & natural language processing

> From now on I will consider a language to be a set (finite or
> infinite) of sentences, each finite in length and constructed out of a
> finite set of elements. All natural languages in their spoken or
> written form are languages in this sense.
>
> Noam Chomsky, *Syntactic Structures*

### Learning objectives {.unnumbered}

Students completing this chapter will:

-   Have a realistic understanding of the capabilities of current text
    mining and NLP software;

-   Be able to use R and associated packages for text mining and NLP.

## The nature of language {.unnumbered}

Language enables humans to cooperate through information exchange. We
typically associate language with sound and writing, but gesturing,
which is older than speech, is also a means of collaboration. The
various dialects of sign languages are effective tools for visual
communication. Some species, such as ants and bees, exchange information
using chemical substances known as pheromones. Of all the species,
humans have developed the most complex system for cooperation, starting
with gestures and progressing to digital technology, with language being
the core of our ability to work collectively.

Natural language processing (NLP) focuses on developing and implementing
software that enables computers to handle large scale processing of
language in a variety of forms, such as written and spoken. While it is
a relatively easy task for computers to process numeric information,
language is far more difficult because of the flexibility with which it
is used, even when grammar and syntax are precisely obeyed. There is an
inherent ambiguity of written and spoken speech. For example, the word
"set" can be a noun, verb, or adjective, and the *Oxford English
Dictionary* defines over 40 different meanings. Irregularities in
language, both in its structure and use, and ambiguities in meaning make
NLP a challenging task. Be forewarned. Don't expect NLP to provide the
same level of exactness and starkness as numeric processing. NLP output
can be messy, imprecise, and confusing -- just like the language that
goes into an NLP program. One of the well-known maxims of information
processing is "garbage-in, garbage-out." While language is not garbage,
we can certainly observe that "language-in, language-out" is a truism.
You can't start with something that is marginally ambiguous and expect a
computer to turn it into a precise statement. Legal and religious
scholars can spend years learning how to interpret a text and still
reach different conclusions as to meaning.

NLP, despite its limitations, enables humans to process large volumes of
language data (e.g., text) quickly and to identify patterns and features
that might be useful. A well-educated human with domain knowledge
specific to the same data might make more sense of these data, but it
might take months or years. For example, a firm might receive over a
1,000 tweets, 500 Facebook mentions, and 20 blog references in a day. It
needs NLP to identify within minutes or hours which of these many
messages might need human action.

Text mining and NLP overlap in their capabilities and goals. The
ultimate objective is to extract useful and valuable information from
text using analytical methods and NLP. Simply counting words in a
document is a an example of text mining because it requires minimal NLP
technology, other than separating text into words. Whereas, recognizing
entities in a document requires prior extensive machine learning and
more intensive NLP knowledge. Whether you call it text mining or NLP,
you are processing natural language. We will use the terms somewhat
interchangeably in this chapter.

The human brain has a special capability for learning and processing
languages and reconciling ambiguities,[^textmining-1] and it is a skill
we have yet to transfer to computers. NLP can be a good servant, but
enter its realm with realistic expectations of what is achievable with
the current state-of-the-art.

[^textmining-1]: Ambiguities are often the inspiration for puns. "You
    can tune a guitar, but you can't tuna fish. Unless of course, you
    play bass," by Douglas Adams

## Levels of processing {.unnumbered}

There are three levels to consider when processing language.

### Semantics {.unnumbered}

Semantics focuses on the meaning of words and the interactions between
words to form larger units of meaning (such as sentences). Words in
isolation often provide little information. We normally need to read or
hear a sentence to understand the sender's intent. One word can change
the meaning of a sentence (e.g., "Help needed versus Help not needed").
It is typically an entire sentence that conveys meaning. Of course,
elaborate ideas or commands can require many sentences.

### Discourse {.unnumbered}

Building on semantic analysis, discourse analysis aims to determine the
relationships between sentences in a communication, such as a
conversation, consisting of multiple sentences in a particular order.
Most human communications are a series of connected sentences that
collectively disclose the sender's goals. Typically, interspersed in a
conversation are one or more sentences from one or more receivers as
they try to understand the sender's purpose and maybe interject their
thoughts and goals into the discussion. The points and counterpoints of
a blog are an example of such a discourse. As you might imagine, making
sense of discourse is frequently more difficult, for both humans and
machines, than comprehending a single sentence. However, the braiding of
question and answer in a discourse, can sometimes help to reduce
ambiguity.

### Pragmatics {.unnumbered}

Finally, pragmatics studies how context, world knowledge, language
conventions, and other abstract properties contribute to the meaning of
human conversation. Our shared experiences and knowledge often help us
to make sense of situations. We derive meaning from the manner of the
discourse, where it takes place, its time and length, who else is
involved, and so forth. Thus, we usually find it much easier to
communicate with those with whom we share a common culture, history, and
socioeconomic status because the great collection of knowledge we
jointly share assists in overcoming ambiguity.

## Tokenization {.unnumbered}

Tokenization is the process of breaking a document into chunks (e.g.,
words), which are called tokens. Whitespaces (e.g., spaces and tabs) are
used to determine where a break occurs. Tokenization typically creates a
*bag of words* for subsequent processing. Many text mining functions use
words as the foundation for analysis.

### Counting words {.unnumbered}

To count the number of words in a string, simply count the number of
times there are one or more consecutive spaces using the pattern
"[:space:]+" and then add one, because the last word is not followed by
a space.

```{r counting, eval = TRUE}
library(stringr)
str_count("The dead batteries were given out free of charge", "[:space:]+") + 1
```

## Sentiment analysis {.unnumbered}

Sentiment analysis is a popular and simple method of measuring aggregate
feeling. In its simplest form, it is computed by giving a score of +1 to
each "positive" word and -1 to each "negative" word and summing the
total to get a sentiment score. A text is decomposed into words. Each
word is then checked against a list to find its score (i.e., +1 or -1),
and if the word is not in the list, it doesn't score.

A major shortcoming of sentiment analysis is that irony (e.g., "The name
of Britain's biggest dog (until it died) was Tiny") and sarcasm (e.g.,
"I started out with nothing and still have most of it left") are usually
misclassified. Also, a phrase such as "not happy" might be scored as +1
by a sentiment analysis program that simply examines each word and not
those around it.

The **sentimentr package** offers an advanced implementation of
sentiment analysis. It is based on a polarity table, in which a word and
its polarity score (e.g., -1 for a negative word) are recorded. The
default polarity table is provided by the syuzhet package. You can
create a polarity table suitable for your context, and you are not
restricted to 1 or -1 for a word's polarity score. Here are the first
few rows of the default polarity table.

```{r sentiment-1, eval = T, message = F, warning = F}
library(sentimentr)
library(syuzhet)
head(get_sentiment_dictionary())
```

In addition, sentimentr supports valence shifters, which are words that
alter or intensify the meaning of a polarizing word (i.e., a word
appearing in the polarity table) appearing in the text or document under
examination. Each word has a value to indicate how to interpret its
effect (negators (1), amplifiers(2), de-amplifiers (3), and conjunction
(4).

Now, let's see how we use the **sentiment function**. We'll start with
an example that does not use valence shifters, in which case we specify
that the sentiment function should not look for valence words before or
after any polarizing word. We indicate this by setting n.before and
n.after to 0. Our sample text consists of several sentences, as shown in
the following code.

```{r sentiment-2, eval = T}
library(sentimentr)
library(syuzhet)
sample = c("You're awesome and I love you", "I hate and hate and hate.
So angry. Die!", "Impressed and amazed: you are peerless in your
achievement of unparalleled mediocrity.")
sentiment(sample, n.before=0, n.after=0, amplifier.weight=0)
```

Notice that the sentiment function breaks each element (the text between
quotes in this case) into sentences, identifies each sentence in an
element, and computes the word count for each of these sentences. The
sentiment score is the sum of the polarity scores divided by the square
root of the number of words in the associated sentence.

To get the overall sentiment for the sample text, use:

```{r sentiment-3, eval = T}
library(sentimentr)
library(syuzhet)
sample = c("You're awesome and I love you", "I hate and hate and hate.So angry. Die!", "Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.")
y <- sentiment(sample,
n.before=0, n.after=0)
mean(y$sentiment)
```

When a **valence shift** is detected before or after a polarizing word,
its effect is incorporated in the sentiment calculation. The size of the
effect is indicated by the amplifier.weight, a sentiment function
parameter with a value between 0 and 1. The weight amplifies or
de-amplifies by multiplying the polarized terms by 1 + the amplifier
weight. A ***negator*** flips the sign of a polarizing word. A
***conjunction*** amplifies the current clause and down weights the
prior clause. Some examples in the following table illustrate the
results of applying the code to a variety of input text. The polarities
are -1 (crazy) and 1 (love). There is a negator (not), two amplifiers
(very and much), and a conjunction (but). Contractions are treated as
amplifiers and so get weights based on the contraction (.9 in this case)
and amplification (.8) in this case.

```{r sentiment-4, eval = T}
library(sentimentr)
library(syuzhet)
sample = c("You're awesome and I love you", "I hate and hate and hate.
So angry. Die!", "Impressed and amazed: you are peerless in your
achievement of unparalleled mediocrity.")
sentiment(sample, n.before = 2, n.after = 2, amplifier.weight=.8, but.weight = .9)
```

| **Type**     | **Code** | **Text**                                | **Sentiment** |
|:-------------|:---------|:----------------------------------------|:--------------|
|              |          | You're crazy, and I love you.           | 0             |
| Negator      | 1        | You're not crazy, and I love you.       | 0.57          |
| Amplifier    | 2        | You're crazy, and I love you very much. | 0.21          |
| De-amplifier | 3        | You're slightly crazy, and I love you.  | 0.23          |
| Conjunction  | 4        | You're crazy, but I love you.           | 0.45          |

> ❓**Skill builder**
>
> Run the following R code and comment on how sensitive sentiment
> analysis is to the n.before and n.after parameters.

```{r sentiment-5, eval = T}
library(sentimentr) 
library(syuzhet)
sample = c("You're not crazy and I love you very much.")
sentiment(sample, n.before = 4, n.after=3, amplifier.weight=1) 
sentiment(sample, n.before = Inf, n.after=Inf, amplifier.weight=1)
```

> What are the correct polarities for each word, and weights for
> negators, amplifiers and so on? It is a judgment call and one that is
> difficult to justify.

Sentiment analysis has given you an idea of some of the issues
surrounding text mining. Let's now look at the topic in more depth and
explore some of the tools available in **tm**, a general purpose text
mining package for R. We will also use a few other R packages which
support text mining and displaying the results.

## Corpus {.unnumbered}

A collection of text is called a corpus. It is common to use N for the
*corpus size*, the number of tokens, and V for the *vocabulary*, the
number of distinct tokens.

In the following examples, our corpus consists of Warren Buffett's
annual letters to the shareholders of Berkshire Hathaway[^textmining-2]
for the period 1998-2012.[^textmining-3] The letters, available in html
or pdf, were converted to separate text files using Abbyy Fine Reader.
Tables, page numbers, and graphics were removed during the conversion.

[^textmining-2]: <http://www.berkshirehathaway.com/letters/letters.html>

[^textmining-3]: The converted letters are available at
    [<http://www.richardtwatson.com/BuffettLetters/>](http://www.terry.uga.edu/people/rwatson/)
    and will be extended to include earlier and more recent letters. The
    folder also contains the original letter in pdf.

The following R code sets up a loop to read each of the letters and add
it to a data frame. When all the letters have been read, they are turned
into a corpus.

```{r corpus-creation, eval = T, message = FALSE}
library(stringr)
library(tm)
library(NLP)
#set up a data frame to hold up to 100 letters
df <-  data.frame(num=100)
begin <-  1998 # date of first letter
i <-  begin
# read the letters
while (i < 2013) {
 y <- as.character(i)
 # create the file name
 f <- str_c('http://www.richardtwatson.com/BuffettLetters/',y, 'ltr.txt',delim='')
 # read the letter as on large string
 d <-  readChar(f,nchars=1e6)
 # add letter to the data frame
 df[i-begin+1,1] <-  i
 df[i-begin+1,2] <-  d
 i <-  i + 1
}
colnames(df) <- c('doc_id', 'text')
# create the corpus
letters <-  Corpus(DataframeSource(df))
```

## Readability {.unnumbered}

There are several approaches to estimating the readability of a
selection of text. They are usually based on counting the words in each
sentence and the number of syllables in each word. For example, the
Flesch-Kincaid method uses the formula:

```{=tex}
\begin{equation}
(11.8 * syllables\: per\: word) + (0.39 * words\:per\:sentence) - 15.59
\end{equation}
```
It estimates the grade-level or years of education required of the
reader. The bands are:

| Score | Education level |
|:------|:----------------|
| 13-16 | Undergraduate   |
| 16-18 | Masters         |
| 19-   | PhD             |

: Readability score and grade level

The R package koRpus has a number of methods for calculating readability
scores. You first need to tokenize the text using the package's tokenize
function. Then complete the calculation.

```{r readability, eval = T, message = FALSE}
library(koRpus)
library(koRpus.lang.en)
library(sylly)
#tokenize the first letter in the corpus after converting to character vector
txt <-  letters[[1]][1] # first element in the list
tagged.text <- tokenize(as.character(txt),format="obj",lang="en")
# score
readability(tagged.text, hyphen=NULL,index="FORCAST", quiet = TRUE)
```

## Preprocessing {.unnumbered}

Before commencing analysis, a text file typically needs to be prepared
for processing. Several steps are usually taken.

### Case conversion {.unnumbered}

For comparison purposes, all text should be of the same case.
Conventionally, the choice is to convert to all lower case. First convert to UTF-8.

```{r case, eval = T, message = FALSE}
content_transformer <- function(x) iconv(x, to='UTF-8-MAC', sub='byte')
clean.letters <- tm_map(letters, content_transformer)
clean.letters <- tm_map(letters,tolower)
```

### Punctuation removal {.unnumbered}

Punctuation is usually removed when the focus is just on the words in a
text and not on higher level elements such as sentences and paragraphs.

```{r punctuation, eval = T, message = FALSE}
clean.letters <- tm_map(clean.letters,removePunctuation)
```

### Number removal {.unnumbered}

You might also want to remove all numbers.

```{r numbers, eval = T, message = FALSE}
clean.letters <- tm_map(clean.letters,removeNumbers)
```

### Stripping extra white spaces {.unnumbered}

Removing extra spaces, tabs, and such is another common preprocessing
action.

```{r stripping, eval = T, message = FALSE}
clean.letters <- tm_map(clean.letters,stripWhitespace)
```

> ❓**Skill builder**
>
> Redo the readability calculation after executing the preprocessing
> steps described in the previous section. What do you observe?

### Stop word filtering {.unnumbered}

Stop words are short common words that can be removed from a text
without affecting the results of an analysis. Though there is no
commonly agreed upon list of stop works, typically included are *the,
is, be, and, but, to,* and *on*. Stop word lists are typically all
lowercase, thus you should convert to lowercase before removing stop
words. Each language has a set of stop words. In the following sample
code, we use the SMART list of English stop words.[^textmining-4]

[^textmining-4]: <http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop>

```{r filtering, eval = T, message = FALSE}
clean.letters <- tm_map(clean.letters,removeWords,stopwords("SMART"))
```

### Specific word removal {.unnumbered}

There can also specify particular words to be removed via a character
vector. For instance, you might not be interested in tracking references
to *Berkshire Hathaway* in Buffett's letters. You can set up a
dictionary with words to be removed from the corpus.

```{r dictonary, eval = T, message = FALSE}
dictionary <- c("berkshire","hathaway", "million", "billion", "dollar")
clean.letters <- tm_map(clean.letters,removeWords,dictionary)
```

### Word length filtering {.unnumbered}

You can also apply a filter to remove all words less than or greater
than a specified lengths. The tm package provides this option when
generating a term frequency matrix, something you will read about
shortly.

### Parts of speech (POS) filtering {.unnumbered}

Another option is to remove particular types of words. For example, you
might scrub all adverbs and adjectives.

### Stemming {.unnumbered}

Stemming reduces inflected or derived words to their stem or root form.
For example, *cats* and *catty* stem to *cat*. *Fishlike* and *fishy*
stem to *fish*. As a stemmer generally works by suffix stripping, so
*catfish* should stem to *cat*. As you would expect, stemmers are
available for different languages, and thus the language must be
specified.

```{r stemming-1, eval = T, message = FALSE}
# stem the document -- might take a while to run
stem.letters <- tm_map(clean.letters,stemDocument, language =
"english")
```

Following stemming, you can apply **stem completion** to return stems to
their original form to make the text more readable. The original
document that was stemmed, in this case, is used as the dictionary to
search for possible completions. Stem completion can apply several
different rules for converting a stem to a word, including "prevalent"
for the most frequent match, "first" for the first found completion, and
"longest" and "shortest" for the longest and shortest, respectively,
completion in terms of characters

```{r stemming-2, eval = T, message = FALSE}
# stem completion -- might take a while to run
stem.letters <-  tm_map(clean.letters,stemDocument, language = "english")
```

### Regex filtering {.unnumbered}

The power of regex (regular expressions) can also be used for filtering
text or searching and replacing text. You might recall that we covered
regex when learning SQL.

## Word frequency analysis {.unnumbered}

Word frequency analysis is a simple technique that can also be the
foundation for other analyses. The method is based on creating a matrix
in one of two forms.

A **term-document matrix** contains one row for each term and one column
for each document.

```{r tdm,eval = T, message = FALSE}
tdm <-  TermDocumentMatrix(stem.letters,control = list(minWordLength=3))
dim(tdm)
# report those words occurring more than 100 times
findFreqTerms(tdm, lowfreq = 100, highfreq = Inf)
```

A **document-term matrix** contains one row for each document and one
column for each term.

```{r dtm, eval = T, message = FALSE}
dtm <- DocumentTermMatrix(stem.letters,control = list(minWordLength=3))
dim(dtm)
```

The function dtm() reports the number of distinct terms, the vocabulary,
and the number of documents in the corpus.

### Term frequency {.unnumbered}

Words that occur frequently within a document are usually a good
indicator of the document's content. Term frequency (tf) measures word
frequency.

tf~td~ = number of times term t occurs in document d.

Here is the R code for determining the frequency of words in a corpus.

```{r frequency, eval = T, message = FALSE}
tdm <-  TermDocumentMatrix(stem.letters,control = list(minWordLength=3))
# convert term document matrix to a regular matrix to get frequencies of words
m <-  as.matrix(tdm)
# sort on frequency of terms
v <- sort(rowSums(m), decreasing=TRUE)
# display the ten most frequent words
v[1:10]
```

A probability density plot shows the distribution of words in a document
visually. As you can see, there is a very long and thin tail because a
very small number of words occur frequently. Note that this plot shows
the distribution of words after the removal of stop words.

```{r density-plot, eval = T, message = FALSE, fig.cap = 'Probability density plot of word frequency'}
library(ggplot2)
# get the names corresponding to the words
names <- names(v)
# create a data frame for plotting
d <- data.frame(word=names, freq=v)
ggplot(d,aes(freq)) + 
  geom_density(fill="salmon") + 
  xlab("Frequency") +
  ylab("Density")
```

A word cloud is way of visualizing the most frequent words.

```{r cloud, eval = T, message = FALSE, fig.cap = 'A word cloud'}
library(wordcloud)
library(RColorBrewer)
# select the color palette
pal = brewer.pal(5,"Accent")
# generate the cloud based on the 30 most frequent words
wordcloud(d$word, d$freq, min.freq=d$freq[30],colors=pal)
```

> ❓**Skill builder**
>
> Start with the original letters corpus (i.e., prior to preprocessing)
> and identify the 20 most common words and create a word cloud for
> these words.

## Co-occurrence and association {.unnumbered}

Co-occurrence measures the frequency with which two words appear
together. In the case of document level association, if the two words
both appear or neither appears, then the correlation or association is
1. If two words never appear together in the same document, their
association is -1.

A simple example illustrates the concept. The following code sets up a
corpus of five elementary documents.

```{r co-occurence, eval = T, message = FALSE}
library(tm)
data <-  c("word1", "word1 word2","word1 word2 word3","word1 word2 word3 word4","word1 word2 word3 word4 word5")
corpus <-  VCorpus(VectorSource(data))
tdm <-  TermDocumentMatrix(corpus)
as.matrix(tdm)
```

We compute the correlation of rows to get a measure of association
across documents.

```{r correlation2, eval = T, message = FALSE}
# Correlation between word2 and word3, word4, and word5
cor(c(0,1,1,1,1),c(0,0,1,1,1))
cor(c(0,1,1,1,1),c(0,0,0,1,1))
cor(c(0,1,1,1,1),c(0,0,0,0,1))
```

Alternatively, use the findAssocs function, which computes all
correlations between a given term and all terms in the term-document
matrix and reports those higher than the correlation threshold.

```{r association-1, eval = T, message = FALSE}
library(tm)
# find associations greater than 0.1
findAssocs(tdm,"word2",0.1)
```

Now that you have an understanding of how association works across
documents, here is an example for the corpus of Buffett letters.

```{r association-2, eval = T, message = FALSE}
# Select the first ten letters
tdm <- TermDocumentMatrix(stem.letters[1:10])
# compute the associations
findAssocs(tdm, "invest",0.80)
```

## Cluster analysis {.unnumbered}

Cluster analysis is a statistical technique for grouping together sets
of observations that share common characteristics. Objects assigned to
the same group are more similar in some way than those allocated to
another cluster. In the case of a corpus, cluster analysis groups
documents based on their similarity. Google, for instance, uses
clustering for its news site.

The general principle of cluster analysis is to map a set of
observations in multidimensional space. For example, if you have seven
measures for each observation, each will be mapped into
seven-dimensional space. Observations that are close together in this
space will be grouped together. In the case of a corpus, cluster
analysis is based on mapping frequently occurring words into a
multidimensional space. The frequency with which each word appears in a
document is used as a weight, so that frequently occurring words have
more influence than others.

There are multiple statistical techniques for clustering, and multiple
methods for calculating the distance between points. Furthermore, the
analyst has to decide how many clusters to create. Thus, cluster
analysis requires some judgment and experimentation to develop a
meaningful set of groups.

The following code computes all possible clusters using the Ward method
of cluster analysis. A term-document matrix is sparse, which means it
consists mainly of zeroes. In other words, many terms occur in only one
or two documents, and the cell entries for the remaining documents are
zero. In order to reduce the computations required, sparse terms are
removed from the matrix. You can vary the sparse parameter to see how
the clusters vary.

```{r cluster, eval = T, message = FALSE, fig.cap = 'Dendrogram for Buffett letters from 1998-2012'}
# Cluster analysis
# name the columns for the letter's year
tdm <- TermDocumentMatrix(stem.letters[1:15])
colnames(tdm) <-  1998:2012
# Remove sparse terms
tdm1 <- removeSparseTerms(tdm, 0.5) 
# transpose the matrix
tdmtranspose <-  t(tdm1) 
cluster = hclust(dist(tdmtranspose))
# plot the tree
plot(cluster)
```

The cluster analysis is shown in the following figure as a dendrogram, a
tree diagram, with a leaf for each year. Clusters seem to from around
consecutive years. Can you think of an explanation?

## Topic modeling {.unnumbered}

Topic modeling is a set of statistical techniques for identifying the
themes that occur in a document set. The key assumption is that a
document on a particular topic will contain words that identify that
topic. For example, a report on gold mining will likely contain words
such as "gold" and "ore." Whereas, a document on France, would likely
contain the terms "France," "French," and "Paris."

The package **topicmodels** implements topic modeling techniques within
the R framework. It extends tm to provide support for topic modeling. It
implements two methods: latent Dirichlet allocation (LDA), which assumes
topics are uncorrelated; and correlated topic models (CTM), an extension
of LDA that permits correlation between topics.[^textmining-5] Both LDA
and CTM require that the number of topics to be extracted is determined
a priori. For example, you might decide in advance that five topics
gives a reasonable spread and is sufficiently small for the diversity to
be understood.[^textmining-6]

[^textmining-5]: The mathematics of LDA and CTM are beyond the scope of
    this text. For details, see Grün, B., & Hornik, K. (2011).
    Topicmodels: An R package for fitting topic models. *Journal of
    Statistical Software*, 40(13), 1-30. 

[^textmining-6]: Humans have a capacity to handle about 7±2 concepts at
    a time. Miller, G. A. (1956). The magical number seven, plus or
    minus two: some limits on our capacity for processing information.
    *The Psychological Review*, 63(2), 81-97. 

Words that occur frequently in many documents are not good at
distinguishing among documents. The weighted term frequency inverse
document frequency (tf-idf) is a measure designed for determining which
terms discriminate among documents. It is based on the term frequency
(tf), defined earlier, and the inverse document frequency.

### Inverse document frequency {.unnumbered}

The inverse document frequency (idf) measures the frequency of a term
across documents.

```{=tex}
\begin{equation}

idf_t = log_2\frac{m}{df_t}

\end{equation}
```
> Where
>
> m = number of documents (i.e., rows in the case of a term-document
> matrix);
>
> df~t~ = number of documents containing term *t.*

If a term occurs in every document, then its idf = 0, whereas if a term
occurs in only one document out of 15, its idf = 3.91.

To calculate and display the idf for the letters corpus, we can use the
following R script.

```{r topic-1, eval = T, message = FALSE, fig.cap = 'Inverse document frequency (corpus has 15 documents)'}
# calculate idf for each term
library(slam)
library(ggplot2)
idf <-  log2(nDocs(dtm)/col_sums(dtm > 0))
# create dataframe for graphing
df.idf <-  data.frame(idf)
ggplot(df.idf,aes(idf)) + geom_histogram(fill="chocolate") + xlab("Inverse document frequency")
```

The preceding graphic shows that about 5,000 terms occur in only one
document (i.e., the idf = 3.91) and less than 500 terms occur in every
document. The terms with an idf in the range 1 to 2 are likely to be the
most useful in determining the topic of each document.

### Term frequency inverse document frequency (tf-idf) {.unnumbered}

The weighted term frequency inverse document frequency (tf-idf or ω~td~)
is calculated by multiplying a term's frequency (tf) by its inverse
document frequency (idf).

```{=tex}
\begin{equation}

\omega_{td} = tf_{td}\cdotp log_2\frac{m}{df_t}

\end{equation}
```
Where

tf~td~ = frequency of term *t* in document *d.*

### Topic modeling with R {.unnumbered}

Prior to topic modeling, pre-process a text file in the usual fashion
(e.g., convert to lower case, remove punctuation, and so forth). Then,
create a document-term matrix.

The mean term frequency-inverse document frequency (tf-idf) is used to
select the vocabulary for topic modeling. We use the median value of
tf-idf for all terms as a threshold.

```{r topic-2, eval = T, message = FALSE}
library(topicmodels)
library(slam)
# calculate tf-idf for each term
tfidf <-  tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
# report dimensions (terms)
dim(tfidf)
# report median to use as cut-off point
median(tfidf)
```

The goal of topic modeling is to find those terms that distinguish a
document set. Thus, terms with low frequency should be omitted because
they don't occur often enough to define a topic. Similarly, those terms
occurring in many documents don't differentiate between documents.

A common heuristic is to select those terms with a tf-idf \>
median(tf-idf). As a result, we reduce the document-term matrix by
keeping only those terms above the threshold and eliminating rows that
have zero terms. Because the median is a measure of central tendency,
this approach reduces the number of columns by roughly half.

```{r topic-3, eval = T, message = FALSE}
# select columns with tf-idf > median
dtm <- dtm[,tfidf >= median(tfidf)]
#select rows with rowsum > 0
dtm <- dtm[row_sums(dtm) > 0,]
# report reduced dimension
dim(dtm)
```

As mentioned earlier, the topic modeling method assumes a set number of
topics, and it is the responsibility of the analyst to estimate the
correct number of topics to extract. It is common practice to fit models
with a varying number of topics, and use the various results to
establish a good choice for the number of topics. The analyst will
typically review the output of several models and make a judgment on
which model appears to provide a realistic set of distinct topics. Here
is some code that starts with five topics.

```{r topic-4, eval = T, message = FALSE}
# set number of topics to extract
k <- 5 # number of topics 
SEED <- 2010 # seed for initializing the model rather than the default random
# try multiple methods – takes a while for a big corpus
TM <- list(VEM = LDA(dtm, k = k, control = list(seed = SEED)),  
VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)), 
Gibbs = LDA(dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,  thin = 100, iter = 1000)), CTM = CTM(dtm, k = k,control = list(seed = SEED, var = list(tol = 10^-3), em = list(tol = 10^-3))))
topics(TM[["VEM"]], 1)
terms(TM[["VEM"]], 5)
```

The output indicates that the first three letter (1998-2000) are about
topic 4, the fourth (2001) topic 2, and so on.

Topic 1 is defined by the following terms: thats, bnsf, cant,
blacksholes, and railroad. As we have seen previously, some of these
words (e.g., thats and cant, which we can infer as being that's and
can't) are not useful differentiators, and the dictionary could be
extended to remove them from consideration and topic modeling repeated.
For this particular case, it might be that Buffett's letters don't vary
much from year to year, and he returns to the same topics in each annual
report.

> ❓**Skill builder**
>
> Experiment with the topicmodels package to identify the topics in
> Buffett's letters. You might need to use the dictionary feature of
> text mining to remove selected words from the corpus to develop a
> meaningful distinction between topics.

## Named-entity recognition (NER) {.unnumbered}

Named-entity recognition (NER) places terms in a corpus into predefined
categories such as the names of persons, organizations, locations,
expressions of times, quantities, monetary values, and percentages. It
identifies some or all mentions of these categories, as shown in the
following figure, where an organization, place, and date are recognized.

Named-entity recognition example

![](Figures/Chapter%2017/name-entity-recognition.png)

Tags are added to the corpus to denote the category of the terms
identified.

The \<organization\>Olympics\</organization\> were in
\<place\>London\</place\> in \<date\>2012\</date\>.

There are two approaches to developing an NER capability. A
*rules-based* approach works well for a well-understood domain, but it
requires maintenance and is language dependent. *Statistical
classifiers*, based on machine learning, look at each word in a sentence
to decide whether it is the start of a named-entity, a continuation of
an already identified named-entity, or not part of a named-entity. Once
a named-entity is distinguished, its category (e.g., place) must be
identified and surrounding tags inserted.

Statistical classifiers need to be trained on a large collection of
human-annotated text that can be used as input to machine learning
software. Human-annotation, while time-consuming, does not require a
high level of skill. It is the sort of task that is easily parallelized
and distributed to a large number of human coders who have a reasonable
understanding of the corpus's context (e.g., able to recognize that
London is a place and that the Olympics is an organization). The
software classifier need to be trained on approximately 30,000 words.

The accuracy of NER is dependent on the corpus used for training and the
domain of the documents to be classified. For example, NER is based on a
collection of news stories and is unlikely to be very accurate for
recognizing entities in medical or scientific literature. Thus, for some
domains, you will likely need to annotate a set of sample documents to
create a relevant model. Of course, as times change, it might be
necessary to add new annotated text to the learning script to
accommodate new organizations, place, people and so forth. A
well-trained statistical classifier applied appropriately is usually
capable of correctly recognizing entities with 90 percent accuracy.

### NER software {.unnumbered}

OpenNLP[^textmining-7] is an Apache Java-based machine learning based
toolkit for the processing of natural language in text format. It is a
collection of natural language processing tools, including a sentence
detector, tokenizer, parts-of-speech(POS)-tagger, syntactic parser, and
named-entity detector. The NER tool can recognize people, locations,
organizations, dates, times. percentages, and money. You will need to
write a Java program to take advantage of the toolkit. The R package,
openNLP, provides an interface to OpenNLP.

[^textmining-7]: <http://opennlp.apache.org>

## Future developments {.unnumbered}

Text mining and natural language processing are developing areas and you
can expect new tools to emerge. Document summarization, relationship
extraction, advanced sentiment analysis, and cross-language information
retrieval (e.g., a Chinese speaker querying English documents and
getting a Chinese translation of the search and selected documents) are
all areas of research that will likely result in generally available
software with possible R versions. If you work in this area, you will
need to continually scan for new software that extends the power of
existing methods and adds new text mining capabilities.

## Summary {.unnumbered}

Language enables cooperation through information exchange. Natural
language processing (NLP) focuses on developing and implementing
software that enables computers to handle large scale processing of
language in a variety of forms, such as written and spoken. The inherent
ambiguity in written and spoken speech makes NLP challenging. Don't
expect NLP to provide the same level of exactness and starkness as
numeric processing. There are three levels to consider when processing
language: semantics, discourse, and pragmatics.

Sentiment analysis is a popular and simple method of measuring aggregate
feeling. Tokenization is the process of breaking a document into chunks.
A collection of text is called a corpus. The Flesch-Kincaid formula is a
common way of assessing readability. Preprocessing, which prepares a
corpus for text mining, can include case conversion, punctuation
removal, number removal, stripping extra white spaces, stop word
filtering, specific word removal, word length filtering, parts of speech
(POS) filtering, Stemming, and regex filtering.

Word frequency analysis is a simple technique that can also be the
foundation for other analyses. A term-document matrix contains one row
for each term and one column for each document. A document-term matrix
contains one row for each document and one column for each term. Words
that occur frequently within a document are usually a good indicator of
the document's content. A word cloud is way of visualizing the most
frequent words. Co-occurrence measures the frequency with which two
words appear together. Cluster analysis is a statistical technique for
grouping together sets of observations that share common
characteristics. Topic modeling is a set of statistical techniques for
identifying the topics that occur in a document set. The inverse
document frequency (idf) measures the frequency of a term across
documents. Named-entity recognition (NER) places terms in a corpus into
predefined categories such as the names of persons, organizations,
locations, expressions of times, quantities, monetary values, and
percentages. Statistical classification is used for NER. OpenNLP is an
Apache Java-based machine learning-based toolkit for the processing of
natural language in text format. Document summarization, relationship
extraction, advanced sentiment analysis, and cross-language information
retrieval are all areas of research.

## Key terms and concepts {.unnumbered}

|                                   |                                           |
|-----------------------------------|-------------------------------------------|
| Association                       | Readability                               |
| Cluster analysis                  | Regex filtering                           |
| Co-occurrence                     | Sentiment analysis                        |
| Corpus                            | Statistical classification                |
| Dendrogram                        | Stemming                                  |
| Document-term matrix              | Stop word filtering                       |
| Flesch-Kincaid formula            | Stripping extra white spaces              |
| Inverse document frequency        | Term-document matrix                      |
| KNIME                             | Term frequency                            |
| Named-entity recognition (NER)    | Term frequency inverse document frequency |
| Natural language processing (NLP) | Text mining                               |
| Number removal                    | Tokenization                              |
| OpenNLP                           | Topic modeling                            |
| Parts of speech (POS) filtering   | Word cloud                                |
| Preprocessing                     | Word frequency analysis                   |
| Punctuation removal               | Word length filtering                     |

## References {.unnumbered}

Feinerer, I. (2008). An introduction to text mining in R. *R News*,
8(2), 19-22.

Feinerer, I., Hornik, K., & Meyer, D. (2008). Text mining infrastructure
in R. *Journal of Statistical Software*, 25(5), 1-54.

Grün, B., & Hornik, K. (2011). topicmodels: An R package for fitting
topic models. *Journal of Statistical Software*, 40(13), 1-30.

Ingersoll, G., Morton, T., & Farris, L. (2012). *Taming Text: How to
find, organize and manipulate it*. Greenwich, CT: Manning Publications.

## Exercises {.unnumbered}

1.  Take the recent annual reports for UPS[^textmining-8] and convert
    them to text using an online service, such as
    [[http://www.fileformat.info/convert/doc/pdf2txt.htm]{.ul}](http://convertonlinefree.com/PDFToTXTEN.aspx).
    Complete the following tasks:

    1.  Count the words in the most recent annual report.

    2.  Compute the readability of the most recent annual report.

    3.  Create a corpus.

    4.  Preprocess the corpus.

    5.  Create a term-document matrix and compute the frequency of words
        in the corpus.

    6.  Construct a word cloud for the 25 most common words.

    7.  Undertake a cluster analysis, identify which reports are similar
        in nature, and see if you can explain why some reports are in
        different clusters.

    8.  Build a topic model for the annual reports.

2.  Merge the annual reports for Berkshire Hathaway (i.e., Buffett's
    letters) and UPS into a single corpus.

    1.  Undertake a cluster analysis and identify which reports are
        similar in nature.
    2.  Build a topic model for the combined annual reports.
    3.  Do the cluster analysis and topic model suggest considerable
        differences in the two sets of reports?

[^textmining-8]: <http://www.investors.ups.com/phoenix.zhtml?c=62900&p=irol-reportsannual>